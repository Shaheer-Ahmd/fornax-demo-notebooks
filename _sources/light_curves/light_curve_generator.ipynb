{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa3e824",
   "metadata": {},
   "source": [
    "# Make Multiwavelength Light Curves Using Archival Data\n",
    "***\n",
    "\n",
    "## Learning Goals    \n",
    "By the end of this tutorial, you will be able to:  \n",
    "  &bull; Automatically load a catalog of sources  \n",
    "  &bull; Automatically & efficiently search NASA and non-NASA resources for light curves at scale  \n",
    "  &bull; Store & manipulate light curves in a Pandas multiindex dataframe  \n",
    "  &bull; Plot all light curves on the same plot\n",
    " \n",
    " \n",
    "## Introduction:  \n",
    " &bull; A user has a sample of interesting targets for which they would like to see a plot of available archival light curves.  We start with a small set of changing look AGN from Yang et al., 2018, which are automatically downloaded. Changing look AGN are cases where the broad emission lines appear or disappear (and not just that the flux is variable). \n",
    " \n",
    " &bull; We model light curve plots after van Velzen et al. 2021.  We search through a curated list of time-domain NASA holdings as well as non-NASA sources.  HEASARC catalogs used are Fermi and Beppo-Sax, IRSA catalogs used are ZTF and WISE, and MAST catalogs used are Pan-Starrs, TESS, Kepler, and K2.  Non-NASA sources are Gaia and IceCube. This list is generalized enough to include many types of targets to make this notebook interesting for many types of science.  All of these time-domain archives are searched in an automated and efficient fashion using astroquery, pyvo, pyarrow or APIs.\n",
    " \n",
    " &bull; Light curve data storage is a tricky problem.  Currently we are using a multi-index Pandas dataframe, as the best existing choice for right now.  One downside is that we need to manually track the units of flux and time instead of relying on an astropy storage scheme which would be able to do some of the units worrying for us (even astropy can't do all magnitude to flux conversions).  Astropy does not currently have a good option for multi-band light curve storage.\n",
    " \n",
    " &bull; ML work using these time-series light curves is in two neighboring notebooks: ML_AGNzoo and lc_classifier.\n",
    " \n",
    "## Input:\n",
    " &bull; choose from a list of known changing look AGN from the literature  \n",
    "  OR -    \n",
    " &bull; input your own sample\n",
    "\n",
    "## Output:\n",
    " &bull; an archival optical + IR + neutrino light curve  \n",
    " \n",
    "## Imports:\n",
    " &bull; `acstools` to work with HST magnitude to flux conversion  \n",
    " &bull; `astropy` to work with coordinates/units and data structures  \n",
    " &bull; `astroquery` to interface with archives APIs  \n",
    " &bull; `hpgeom` to locate coordinates in HEALPix space  \n",
    " &bull; `lightkurve` to search TESSS, Kepler, and K2 archives  \n",
    " &bull; `matplotlib` for plotting  \n",
    " &bull; `multiprocessing` to use the power of multiple CPUs to get work done faster  \n",
    " &bull; `numpy` for numerical processing  \n",
    " &bull; `pandas` for their data structure DataFrame and all the accompanying functions  \n",
    " &bull; `pyarrow` to work with Parquet files for WISE and ZTF  \n",
    " &bull; `pyvo` for acessing Virtual Observatory(VO) standard data  \n",
    " &bull; `requests` to get information from URLs  \n",
    " &bull; `s3fs` to connect to AWS S3 buckets  \n",
    " &bull; `scipy` to do statistics  \n",
    " &bull; `tqdm` to track progress on long running jobs  \n",
    " &bull; `urllib` to handle archive searches with website interface\n",
    "\n",
    "## Authors:\n",
    "Jessica Krick, Shoubaneh Hemmati, Andreas Faisst, Troy Raen, Brigitta Sip≈ëcz, Dave Shupe\n",
    "\n",
    "## Acknowledgements:\n",
    "Suvi Gezari, Antara Basu-zych, Stephanie LaMassa  \n",
    "MAST, HEASARC, & IRSA Fornax teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all dependencies are installed\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e2b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import astropy.units as u\n",
    "import pandas as pd\n",
    "from astropy.table import Table\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# local code imports\n",
    "sys.path.append('code_src/')\n",
    "from data_structures import MultiIndexDFObject\n",
    "from gaia_functions import Gaia_get_lightcurve\n",
    "from HCV_functions import HCV_get_lightcurves\n",
    "from heasarc_functions import HEASARC_get_lightcurves\n",
    "from icecube_functions import Icecube_get_lightcurve\n",
    "from panstarrs import Panstarrs_get_lightcurves\n",
    "from plot_functions import create_figures\n",
    "from sample_selection import (clean_sample, get_green_sample, get_hon_sample, get_lamassa_sample, get_lopeznavas_sample,\n",
    "    get_lyu_sample, get_macleod16_sample, get_macleod19_sample, get_ruan_sample, get_SDSS_sample, get_sheng_sample, get_yang_sample)\n",
    "from TESS_Kepler_functions import TESS_Kepler_get_lightcurves\n",
    "# Note: WISE and ZTF data are temporarily located in a non-public AWS S3 bucket. It is automatically\n",
    "# available from the Fornax SMCE, but will require user credentials for access outside the SMCE.\n",
    "from WISE_functions import WISE_get_lightcurves\n",
    "from ztf_functions import ZTF_get_lightcurve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedefd92",
   "metadata": {},
   "source": [
    "## 1. Define the sample\n",
    "We define here a \"gold\" sample of spectroscopically confirmed changing look AGN and quasars. This sample includes both objects which change from type 1 to type 2 and also the opposite.  Future studies may want to treat these as seperate objects or seperate QSOs from AGN.  Bibcodes for the samples used are listed next to their functions for reference.  \n",
    " \n",
    "Significant work went into the functions which grab the samples from the papers.  They use Astroquery, NED, SIMBAD, Vizier, and in a few cases grab the tables from the html versions of the paper.  There are trickeries involved in accessing coordinates from tables in the literature. Not every literature table is stored in its entirety in all of these resrources, so be sure to check that your chosen method is actually getting the information that you see in the paper table.  Warning: You will get false results if using NED or SIMBAD on a table that has more rows than are printed in the journal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up the sample\n",
    "# Initially set up lists to hold the coordinates and their reference paper name as a label\n",
    "coords =[]\n",
    "labels = []\n",
    "\n",
    "# Choose your own adventure:\n",
    "\n",
    "#get_lamassa_sample(coords, labels)  #2015ApJ...800..144L\n",
    "#get_macleod16_sample(coords, labels) #2016MNRAS.457..389M\n",
    "#get_ruan_sample(coords, labels) #2016ApJ...826..188R\n",
    "#get_macleod19_sample(coords, labels)  #2019ApJ...874....8M\n",
    "#get_sheng_sample(coords, labels)  #2020ApJ...889...46S\n",
    "#get_green_sample(coords, labels)  #2022ApJ...933..180G\n",
    "#get_lyu_sample(coords, labels)  #z32022ApJ...927..227L\n",
    "#get_lopeznavas_sample(coords, labels)  #2022MNRAS.513L..57L\n",
    "#get_hon_sample(coords, labels)  #2022MNRAS.511...54H\n",
    "get_yang_sample(coords, labels)   #2018ApJ...862..109Y\n",
    "\n",
    "# Get some \"normal\" QSOs \n",
    "# there are ~500K of these, so choose the number based on\n",
    "# a balance between speed of running the light curves and whatever \n",
    "# the ML algorithms would like to have\n",
    "\n",
    "#num_normal_QSO = 5000\n",
    "#get_SDSS_sample(coords, labels, num_normal_QSO)\n",
    "\n",
    "# Remove duplicates, attach an objectid to the coords,\n",
    "# convert to astropy table to keep all relevant info together\n",
    "sample_table = clean_sample(coords, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27892a",
   "metadata": {},
   "source": [
    "### 1.1 Build your own sample\n",
    "\n",
    "To build your own sample, you can follow the examples of functions above to grab coordinates from your favorite literature resource, \n",
    "\n",
    "or\n",
    "\n",
    "You can use [astropy's read](https://docs.astropy.org/en/stable/io/ascii/read.html) function to read in an input table\n",
    "to an [astropy table](https://docs.astropy.org/en/stable/table/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c4ba65",
   "metadata": {},
   "source": [
    "### 1.2 Write out your sample to disk\n",
    "\n",
    "At this point you may wish to write out your sample to disk and reuse that in future work sessions, instead of creating it from scratch again.\n",
    "\n",
    "For the format of the save file, we would suggest to choose from various formats that fully support astropy objects(eg., SkyCoord).  One example that works is Enhanced Character-Separated Values or ['ecsv'](https://docs.astropy.org/en/stable/io/ascii/ecsv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e32a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_table.write('data/input_sample.ecsv', format='ascii.ecsv', overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933044a",
   "metadata": {},
   "source": [
    "### 1.3 Load the sample table from disk\n",
    "\n",
    "Do only this step from this section when you have a previously generated sample table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b16e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_table = Table.read('data/input_sample.ecsv', format='ascii.ecsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5a11b",
   "metadata": {},
   "source": [
    "### 1.4 Initialize data structure to hold the light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdeb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wrote our own class for a Pandas MultiIndex [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) for storing the light curves\n",
    "# This class helps simplify coding of common uses for the DataFrame.\n",
    "df_lc = MultiIndexDFObject()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264bced",
   "metadata": {},
   "source": [
    "## 2. Find light curves for these targets in NASA catalogs\n",
    "We search a curated list of time-domain catalogs from NASA astrophysics archives.  Because each archive is different, and in many cases each catalog is different, each function to access a catalog is necesarily specialized to the location and format of that particular catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc75904",
   "metadata": {},
   "source": [
    "### 2.1 HEASARC: FERMI & Beppo SAX\n",
    "The function to retrieve HEASARC data accesses the HEASARC archive using a pyvo search with a table upload.  This is the fastest way to access data from HEASARC catalogs at scale.  \n",
    "\n",
    "While these aren't strictly light curves, we would like to track if there are gamma rays detected in advance of any change in the CLAGN light curves. We store these gamma ray detections as single datapoints.  Because gamma ray detections typically have very large error radii, our current technique is to keep matches in the catalogs within some manually selected error radius, currently defaulting to 1 degree for Fermi and 3 degrees for Beppo SAX.  These values are chosen based on a histogram of all values for those catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f4b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_serial = time.time()  #keep track of all serial archive calls to compare later with parallel archive call time\n",
    "heasarcstarttime = time.time()\n",
    "\n",
    "# What is the size of error_radius for the catalogs that we will accept for our cross-matching?\n",
    "# in degrees; chosen based on histogram of all values for these catalogs\n",
    "max_fermi_error_radius = str(1.0)  \n",
    "max_sax_error_radius = str(3.0)\n",
    "\n",
    "# List of missions to query and their corresponding error radii\n",
    "heasarc_cat = [\"FERMIGTRIG\", \"SAXGRBMGRB\"]\n",
    "error_radius = [max_fermi_error_radius , max_sax_error_radius]\n",
    "\n",
    "# get heasarc light curves in the above curated list of missions \n",
    "df_lc_fermi = HEASARC_get_lightcurves(sample_table, heasarc_cat, error_radius)\n",
    "\n",
    "# add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_fermi)\n",
    "\n",
    "print('heasarc search took:', time.time() - heasarcstarttime, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5b2e3c",
   "metadata": {},
   "source": [
    "### 2.2 IRSA: ZTF\n",
    "The function to retrieve ZTF light curves accesses a parquet version of the ZTF catalog stored in the cloud using pyarrow.  This is the fastest way to access the ZTF catalog at scale.  The ZTF [API](https://irsa.ipac.caltech.edu/docs/program_interface/ztf_lightcurve_api.html) is available for small sample searches.  One unique thing about this function is that it has parallelization built in to the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df14a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZTFstarttime = time.time()\n",
    "\n",
    "# get ZTF lightcurves\n",
    "# use the nworkers arg to control the amount of parallelization in the data loading step\n",
    "df_lc_ZTF = ZTF_get_lightcurve(sample_table, nworkers=6)\n",
    "\n",
    "# add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_ZTF)\n",
    "\n",
    "print('ZTF search took:', time.time() - ZTFstarttime, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0377eb",
   "metadata": {},
   "source": [
    "### 2.3 IRSA: WISE\n",
    "\n",
    "We use the unWISE light curves catalog ([Meisner et al., 2023](https://ui.adsabs.harvard.edu/abs/2023AJ....165...36M/abstract)) which ties together all WISE & NEOWISE 2010 - 2020 epochs.  Specifically it combines all observations at a single epoch to achieve deeper mag limits than individual observations alone.\n",
    "\n",
    "The function to retrieve WISE light curves accesses an IRSA generated version of the catalog in parquet format being stored in the AWS cloud [Open Data Repository](https://registry.opendata.aws/collab/nasa/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "WISEstarttime = time.time()\n",
    "\n",
    "bandlist = ['W1', 'W2']  #list of the WISE band names\n",
    "WISE_radius = 1.0 * u.arcsec\n",
    "# get WISE light curves\n",
    "df_lc_WISE = WISE_get_lightcurves(sample_table, WISE_radius, bandlist)\n",
    "\n",
    "# add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_WISE)\n",
    "\n",
    "print('WISE search took:', time.time() - WISEstarttime, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d098d",
   "metadata": {},
   "source": [
    "### 2.4 MAST: Pan-STARRS\n",
    "The function to retrieve lightcurves from Pan-STARRS currently uses their API; based on this [example](https://ps1images.stsci.edu/ps1_dr2_api.html).  This search is not efficient at scale and we expect it to be replaced in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a15287",
   "metadata": {},
   "outputs": [],
   "source": [
    "panstarrsstarttime = time.time()\n",
    "\n",
    "panstarrs_search_radius = 1.0/3600.0    # search radius = 1 arcsec\n",
    "# get panstarrs light curves\n",
    "df_lc_panstarrs = Panstarrs_get_lightcurves(sample_table, panstarrs_search_radius)\n",
    "\n",
    "# add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_panstarrs)\n",
    "\n",
    "print('Panstarrs search took:', time.time() - panstarrsstarttime, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbd5d9",
   "metadata": {},
   "source": [
    "### 2.5 MAST: TESS, Kepler and K2\n",
    "The function to retrieve lightcurves from these three missions currently uses the open source package [`lightKurve`](https://docs.lightkurve.org/index.html).  This search is not efficient at scale and we expect it to be replaced in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightkurvestarttime = time.time()\n",
    "\n",
    "TESS_search_radius = 1.0  #arcseconds\n",
    "# get TESS/Kepler/K2 light curves\n",
    "df_lc_TESS = TESS_Kepler_get_lightcurves(sample_table, TESS_search_radius)\n",
    "\n",
    "# add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_TESS)\n",
    "\n",
    "print('TESS/Kepler/K2 search took:', time.time() - lightkurvestarttime, 's')\n",
    "\n",
    "# LightKurve will return an \"Error\" when it doesn't find a match for a target\n",
    "# These are not real errors and can be safely ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a38ce",
   "metadata": {},
   "source": [
    "### 2.6 MAST: Hubble Catalog of Variables ([HCV](https://archive.stsci.edu/hlsp/hcv))\n",
    "The function to retrieve lightcurves from HCV currently uses their API; based on this [example](https://archive.stsci.edu/hst/hsc/help/HCV/HCV_API_demo.html). This search is not efficient at scale and we expect it to be replaced in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbfc4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "HCVstarttime = time.time()\n",
    "\n",
    "HCV_radius = 1.0/3600.0 # radius = 1 arcsec\n",
    "# get HCV light curves\n",
    "df_lc_HCV = HCV_get_lightcurves(sample_table, HCV_radius)\n",
    "\n",
    "# add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_HCV)\n",
    "\n",
    "print('HCV search took:', time.time() - HCVstarttime, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6cf992",
   "metadata": {},
   "source": [
    "## 3. Find light curves for these targets in relevant, non-NASA catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6565f3",
   "metadata": {},
   "source": [
    "### 3.1 Gaia\n",
    "The function to retrieve Gaia light curves accesses the Gaia DR3 \"source lite\" catalog using an astroquery search with a table upload to do the join with the Gaia phtometry. This is currently the fastest way to access light curves from Gaia at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b886930",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaiastarttime = time.time()\n",
    "\n",
    "# get Gaia light curves\n",
    "df_lc_gaia = Gaia_get_lightcurve(sample_table, 1/3600., 0)\n",
    "\n",
    "# add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_gaia)\n",
    "\n",
    "print('gaia search took:', time.time() - gaiastarttime, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88dbd9f",
   "metadata": {},
   "source": [
    "### 3.3 Icecube neutrinos\n",
    "\n",
    "There are several [catalogs](https://icecube.wisc.edu/data-releases/2021/01/all-sky-point-source-icecube-data-years-2008-2018) (basically one for each year of IceCube data from 2008 - 2018). The following code creates a large catalog by combining\n",
    "all the yearly catalogs.\n",
    "The IceCube catalog contains Neutrino detections with associated energy and time and approximate direction (which is uncertain by half-degree scales....). Usually, for active events only one or two Neutrinos are detected, which makes matching quite different compared to \"photons\". For our purpose, we will list the top 3 events in energy that are within a given distance to the target.\n",
    "\n",
    "This time series (time vs. neutrino energy) information is similar to photometry. We choose to storing time and energy in our data structure, leaving error = 0. What is __not__ stored in this format is the distance or angular uncertainty of the event direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a36df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "icecubestarttime = time.time()\n",
    "\n",
    "# get icecube datapoints\n",
    "df_lc_icecube = Icecube_get_lightcurve(sample_table ,\n",
    "                                   icecube_select_topN = 3)\n",
    "\n",
    "# add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_icecube)\n",
    "\n",
    "print('icecube search took:', time.time() - icecubestarttime, 's')\n",
    "end_serial = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarking\n",
    "print('total time for serial archive calls is ', end_serial - start_serial, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309dde89",
   "metadata": {},
   "source": [
    "## 4. Parallel processing the archive calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some variables in case the above serial cells are not run\n",
    "max_fermi_error_radius = str(1.0)  \n",
    "max_sax_error_radius = str(3.0)\n",
    "heasarc_cat = [\"FERMIGTRIG\", \"SAXGRBMGRB\"]\n",
    "error_radius = [max_fermi_error_radius , max_sax_error_radius]\n",
    "bandlist = [\"W1\", \"W2\"]\n",
    "wise_radius = 1.0 * u.arcsec\n",
    "panstarrs_radius = 1.0 / 3600.0  # search radius = 1 arcsec\n",
    "lk_radius = 1.0  # arcseconds\n",
    "hcv_radius = 1.0 / 3600.0  # radius = 1 arcsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5329102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of workers to use in the parallel processing pool\n",
    "# this should equal the total number of archives called\n",
    "n_workers = 8\n",
    "\n",
    "# \"spawn\" new processes because it uses less memory and is thread safe\n",
    "# in particular, this is required for pd.read_parquet (used by ZTF_get_lightcurve)\n",
    "# https://stackoverflow.com/questions/64095876/multiprocessing-fork-vs-spawn\n",
    "mp.set_start_method(\"spawn\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ZTF call can be parallelized internally, separate from the pool launched below.\n",
    "# these parallelizations are mutually exclusive, so we must turn off the internal parallelization.\n",
    "ztf_nworkers = None\n",
    "\n",
    "# note that the ZTF call is relatively slow compared to other archives.\n",
    "# if you want to query for a large number of objects, it will be faster to call ZTF individually\n",
    "# (code above) and use the internal parallelization. try 8-12 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_starttime = time.time()\n",
    "\n",
    "# start a multiprocessing pool and run all the archive queries\n",
    "parallel_df_lc = MultiIndexDFObject()  # to collect the results\n",
    "callback = parallel_df_lc.append  # will be called once on the result returned by each archive\n",
    "with mp.Pool(processes=n_workers) as pool:\n",
    "\n",
    "    # start the processes that call the archives\n",
    "    pool.apply_async(\n",
    "        Gaia_get_lightcurve, (sample_table, 1/3600., 0), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        HEASARC_get_lightcurves, (sample_table, heasarc_cat, error_radius), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        HCV_get_lightcurves, (sample_table, hcv_radius), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        Icecube_get_lightcurve, (sample_table , 3), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        Panstarrs_get_lightcurves, (sample_table, panstarrs_radius), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        TESS_Kepler_get_lightcurves, (sample_table, lk_radius), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        WISE_get_lightcurves, (sample_table,  wise_radius, bandlist), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        ZTF_get_lightcurve, (sample_table, ztf_nworkers), callback=callback\n",
    "    )\n",
    "\n",
    "    pool.close()  # signal that no more jobs will be submitted to the pool\n",
    "    pool.join()  # wait for all jobs to complete, including the callback\n",
    "\n",
    "parallel_endtime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc1403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long did parallel processing take?\n",
    "# and look at the results\n",
    "print('parallel processing took', parallel_endtime - parallel_starttime, 's')\n",
    "parallel_df_lc.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data for future use with ML notebook\n",
    "#parquet_savename = 'output/df_lc_090723_yang.parquet'\n",
    "#parallel_df_lc.data.to_parquet(parquet_savename)\n",
    "#print(\"file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df354a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could load a previously saved file in order to plot\n",
    "#parquet_loadname = 'output/df_lc_090723_yang.parquet'\n",
    "#parallel_df_lc = MultiIndexDFObject()\n",
    "#parallel_df_lc.data = pd.read_parquet(parquet_loadname)\n",
    "#print(\"file loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee9647",
   "metadata": {},
   "source": [
    "## 5. Make plots of luminosity as a function of time\n",
    "These plots are modelled after [van Velzen et al., 2021](https://arxiv.org/pdf/2111.09391.pdf). We show flux in mJy as a function of time for all available bands for each object. `show_nbr_figures` controls how many plots are actually generated and returned to the screen.  If you choose to save the plots with `save_ouptut`, they will be put in the output directory and labelled by sample number.\n",
    "\n",
    "__Note__ that in the following, we can either plot the results from `df_lc` (from the serial call) or `parallel_df_lc` (from the parallel call). By default (see next cell) the output of the parallel call is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = create_figures(sample_table ,\n",
    "                   df_lc = parallel_df_lc, # either df_lc (serial call) or parallel_df_lc (parallel call)\n",
    "                   show_nbr_figures = 5,  # how many plots do you actually want to see?\n",
    "                   save_output = True ,  # should the resulting plots be saved?\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76724f3c",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "This work made use of:\n",
    "\n",
    "&bull; Astroquery; Ginsburg et al., 2019, 2019AJ....157...98G  \n",
    "&bull; Astropy; Astropy Collaboration 2022, Astropy Collaboration 2018, Astropy Collaboration 2013,    2022ApJ...935..167A, 2018AJ....156..123A, 2013A&A...558A..33A  \n",
    "&bull; Lightkurve; Lightkurve Collaboration 2018, 2018ascl.soft12013L  \n",
    "&bull; acstools; https://zenodo.org/record/7406933#.ZBH1HS-B0eY  \n",
    "&bull; unWISE light curves; Meisner et al., 2023, 2023AJ....165...36M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f18fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.0"
   }
  },
  "kernelspec": {
   "display_name": "science_demo",
   "language": "python",
   "name": "conda-env-science_demo-py"
  },
  "source_map": [
   12,
   67,
   72,
   100,
   107,
   137,
   148,
   156,
   158,
   164,
   166,
   170,
   174,
   179,
   186,
   206,
   211,
   222,
   230,
   242,
   247,
   258,
   263,
   277,
   282,
   293,
   297,
   302,
   312,
   322,
   336,
   339,
   343,
   356,
   367,
   377,
   417,
   424,
   431,
   437,
   444,
   450,
   462
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}