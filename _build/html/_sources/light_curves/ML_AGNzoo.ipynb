{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6333fc",
   "metadata": {},
   "source": [
    "# How do AGNs selected with different techniques compare? \n",
    "\n",
    "By the IPAC Science Platform Team, last edit: Sep 25th, 2024\n",
    "\n",
    "Active Galactic Nuclei (AGNs), some of the most powerful sources in the universe, emit a broad range of electromagnetic radiation, from radio waves to gamma rays. Consequently, there is a wide variety of AGN labels depending on the identification/selection scheme and the presence or absence of certain emissions (e.g., Radio loud/quiet, Quasars, Blazars, Seiferts, Changing looks). According to the unified model, this zoo of labels we see depend on a limited number of parameters, namely the viewing angle, the accretion rate, presence or lack of jets, and perhaps the properties of the host/environment (e.g., [Padovani et al. 2017](https://arxiv.org/pdf/1707.07134.pdf)). Here, we collect archival temporal data and labels from the literature to compare how some of these different labels/selection schemes compare. \n",
    "\n",
    "We use manifold learning and dimensionality reduction to learn the distribution of AGN lightcurves observed with different facilities. We mostly focus on UMAP ([Uniform Manifold Approximation and Projection, McInnes 2020](https://arxiv.org/pdf/1802.03426.pdf)) but also show two SOM ([Self organizing Map, Kohonen 1990](https://ieeexplore.ieee.org/document/58325)) examples. The reduced 2D projections from these two unsupervised ML techniques reveal similarities and overlaps of different selection techniques and coloring the projections with various statistical physical properties (e.g., mean brightness, fractional lightcurve variation) is informative of correlations of the selections technique with physics such as AGN variability. Using different parts of the EM in training (or in building the initial higher dimensional manifold) demonstrates how much information if any is in that part of the data for each labeling scheme, for example whether with ZTF optical light curves alone, we can identify sources with variability in WISE near IR bands. These techniques also have a potential for identifying targets of a specific class or characteristic for future follow up observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c0d05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bsipocz/.pyenv/versions/3.11.0/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 13.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bsipocz/.pyenv/versions/3.11.0/lib/python3.11/site-packages/umap/plot.py:203: NumbaDeprecationWarning: \u001b[1mThe keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit(nopython=False)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -r requirements.txt\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('code_src/')\n",
    "from data_structures import MultiIndexDFObject\n",
    "from ML_utils import unify_lc, stat_bands, autopct_format, combine_bands,\\\n",
    "mean_fractional_variation, normalize_mean_objects, normalize_max_objects, \\\n",
    "normalize_clipmax_objects, shuffle_datalabel, dtw_distance, stretch_small_values_arctan\n",
    "from sample_lc import build_sample\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "try:\n",
    "    import umap\n",
    "    import umap.plot\n",
    "except:\n",
    "    !pip install umap-learn[plot]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('bmh')\n",
    "colors = [\n",
    "    \"#6C907D\",  # Soft Green\n",
    "    \"#B9C3C2\",  # Pale Gray\n",
    "    \"#8C9EFF\",  # Periwinkle Blue\n",
    "    \"#FF6347\",  # Tomato Red\n",
    "    \"#66CDAA\",  # Medium Aquamarine\n",
    "    \"#E6E6FA\",  # Lavender\n",
    "    \"#FAFAD2\",  # Light Goldenrod Yellow\n",
    "    \"#FFD700\",  # Gold\n",
    "]\n",
    "\n",
    "# To build a sample and generate the multiindex lc dataframe the code below is used, which takes long:\n",
    "#!python sample_lc.py\n",
    "#c,l = build_sample() # if coordinates and labels of the sample are needed separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57630bb7",
   "metadata": {},
   "source": [
    "Here we load a parquet file of light curves generated using the multiband_lc notebook. One can build the sample from different sources in the literature and grab the data from archives of interes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b961650",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/df_lc_5k.parquet.gzip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m parquet_loadname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/df_lc_5k.parquet.gzip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m parallel_df_lc \u001b[38;5;241m=\u001b[39m MultiIndexDFObject()\n\u001b[0;32m----> 3\u001b[0m parallel_df_lc\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_loadname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/pandas/io/parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/pandas/io/parquet.py:265\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    263\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    273\u001b[0m         path_or_handle,\n\u001b[1;32m    274\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    278\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/pandas/io/parquet.py:139\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    129\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/df_lc_5k.parquet.gzip'"
     ]
    }
   ],
   "source": [
    "parquet_loadname = 'output/df_lc_5k.parquet.gzip'\n",
    "parallel_df_lc = MultiIndexDFObject()\n",
    "parallel_df_lc.data = pd.read_parquet(parquet_loadname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427406e7",
   "metadata": {},
   "source": [
    "## What is in this sample?\n",
    "\n",
    "To effectively undertake machine learning (ML) in addressing a specific question, it's imperative to have a clear understanding of the data we'll be utilizing. This understanding aids in selecting the appropriate ML approach and, critically, allows for informed and necessary data preprocessing. For example whether a normalization is needed, and what band to choose for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b9699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "objid = parallel_df_lc.data.index.get_level_values('objectid')[:].unique()\n",
    "seen = Counter()\n",
    "\n",
    "# Grouping all changing look AGNs from the literature into one class:\n",
    "cl_labels = ['LaMassa 15','Green 22','Hon 22','Lyu 21','Sheng 20','MacLeod 19','MacLeod 16','Ruan 16','Yang 18','Lopez-Navas 22']\n",
    "for b in objid:\n",
    "    singleobj = parallel_df_lc.data.loc[b,:,:,:]  \n",
    "    label = singleobj.index.unique('label')\n",
    "    if label in cl_labels:\n",
    "        label = ['CL AGN']\n",
    "    if label=='ZTF-Objname':\n",
    "        label= ['TDE']\n",
    "    if label=='Cicco19':\n",
    "        label= ['VLT Cosmos']                \n",
    "    seen.update(label)\n",
    "\n",
    "#changing order of labels in dictionary only for text to be readable on the plot\n",
    "from collections import OrderedDict\n",
    "key_order = ('SDSS','TDE','CL AGN', 'FermiBL', 'VLT Cosmos','Palomar variable 20','WISE-Variable', 'Galex variable 22')\n",
    "new_queue = OrderedDict()\n",
    "for k in key_order:\n",
    "    new_queue[k] = seen[k]\n",
    "    \n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(r'Sample consists of:',size=15)\n",
    "h = plt.pie(new_queue.values(),labels=new_queue.keys(),autopct=autopct_format(new_queue.values()), textprops={'fontsize': 15},startangle=60,  labeldistance=1.1, wedgeprops = { 'linewidth' : 3, 'edgecolor' : 'white' }, colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ee61c",
   "metadata": {},
   "source": [
    "In this particular example, the largest three subsamples are AGNs selected from [gamma ray observations by the Fermi Large Area Telescope](https://ui.adsabs.harvard.edu/abs/2015yCat..18100014A/similar) (with more than 98% blazars), AGNs from the optical spectra by the [SDSS quasar sample DR16Q](https://www.sdss4.org/dr17/algorithms/qso_catalog/) with a criteria on redshift (z<2), and a subset of AGNs selected in MIR WISE bands based on their variability ([csv in data folder credit RChary](https://ui.adsabs.harvard.edu/abs/2019AAS...23333004P/abstract)). We also include some smaller samples from the literature to see where they sit compared to the rest of the population and if they are localized on the 2D projection. These include the Changing Look AGNs from the literature (e.g., [LaMassa et al. 2015](https://ui.adsabs.harvard.edu/abs/2015ApJ...800..144L/abstract), [Lyu et al. 2022](https://ui.adsabs.harvard.edu/abs/2022ApJ...927..227L/abstract), [Hon et al. 2022](https://ui.adsabs.harvard.edu/abs/2022MNRAS.511...54H/abstract)), a sample which showed variability in Galex UV images ([Wasleske et al. 2022](https://ui.adsabs.harvard.edu/abs/2022ApJ...933...37W/abstract)), a sample of variable sources identified in optical Palomar observarions ([Baldassare et al. 2020](https://ui.adsabs.harvard.edu/abs/2020ApJ...896...10B/abstract)), and the optically variable AGNs in the COSMOS field from a three year program on VLT([De Cicco et al. 2019](https://ui.adsabs.harvard.edu/abs/2019A%26A...627A..33D/abstract)). We also include 30 Tidal Disruption Event coordinates identified from ZTF light curves [Hammerstein et al. 2023](https://iopscience.iop.org/article/10.3847/1538-4357/aca283/meta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446a0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = Counter()\n",
    "for b in objid:\n",
    "    singleobj = parallel_df_lc.data.loc[b,:,:,:]  \n",
    "    label = singleobj.index.unique('label')\n",
    "    bands = singleobj.loc[label[0],:,:].index.get_level_values('band')[:].unique()\n",
    "    seen.update(bands)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.title(r'Number of lightcurves in each waveband in this sample:',size=20)\n",
    "h = plt.bar(seen.keys(), seen.values())\n",
    "plt.ylabel(r'#',size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f928029",
   "metadata": {},
   "source": [
    "The histogram shows the number of lightcurves which ended up in the multi-index data frame from each of the archive calls in different wavebands/filters. We note that the IceCube peak should be corrected as it also include non detections in the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadence = dict((el,[]) for el in seen.keys())\n",
    "timerange = dict((el,[]) for el in seen.keys())\n",
    "\n",
    "for b in objid:\n",
    "    singleobj = parallel_df_lc.data.loc[b,:,:,:]  \n",
    "    label = singleobj.index.unique('label')\n",
    "    bband = singleobj.index.unique('band')\n",
    "    for bb in bband:\n",
    "        bands = singleobj.loc[label[0],bb,:].index.get_level_values('time')[:]\n",
    "        #bands.values\n",
    "        #print(bb,len(bands[:]),np.round(bands[:].max()-bands[:].min(),1))    \n",
    "        cadence[bb].append(len(bands[:]))\n",
    "        if bands[:].max()-bands[:].min()>0:\n",
    "            timerange[bb].append(np.round(bands[:].max()-bands[:].min(),1))    \n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.title(r'Time range and cadence covered in each in each waveband averaged over this sample:')\n",
    "for el in cadence.keys():\n",
    "    #print(el,len(cadence[el]),np.mean(cadence[el]),np.std(cadence[el]))\n",
    "    #print(el,len(timerange[el]),np.mean(timerange[el]),np.std(timerange[el]))\n",
    "    plt.scatter(np.mean(timerange[el]),np.mean(cadence[el]),label=el,s=len(timerange[el]))\n",
    "    plt.errorbar(np.mean(timerange[el]),np.mean(cadence[el]),xerr=np.std(timerange[el]),yerr=np.std(cadence[el]),alpha=0.2)\n",
    "    plt.annotate(el,(np.mean(timerange[el]),np.mean(cadence[el])+2),size=12, rotation=40)\n",
    "plt.ylabel(r'Average number of visits',size=20) \n",
    "plt.xlabel(r'Average baseline (days)',size=20) \n",
    "plt.xlim([0,4000])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a3dbdc",
   "metadata": {},
   "source": [
    "While from the histogram plot we see which bands have the highest number of observed lightcurves, what might matter more in finding/selecting variability or changing look in lightcurves is the cadence and the average baseline of observations. For instance, Panstarrs has a large number of lightcurve detections in our sample, but from the figure above we see that the average number of visits and the baseline for those observations are considerably less than ZTF. WISE also shows the longest baseline of observations which is suitable to finding longer term variability in objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6d68e",
   "metadata": {},
   "source": [
    "## Looking at ZTF lightcurves alone\n",
    "\n",
    "We first look at this sample only in ZTF bands which have the largest number of visits. We start by unifying the time grid of the light curves so oobjects with different start time or number of observations can be compared. We do this by interpolation to a new grid. The choice of the grid resolution and baseline is strictly dependent on the input data, in this case ZTF, to preserve as much as possible all the information from the observations. We measure basic statistics and combine the tree observed ZTF bands into one longer array as input to dimensionailty reduction after deciding on normalization. We also do a shuffling of the sample to be sure that the separations of different classes by ML are not simply due to the order they are seen in training (in case it is not done by the ML routine itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7039c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_inlc = ['zg','zr','zi']\n",
    "objects,dobjects,flabels = unify_lc(parallel_df_lc,bands_inlc,xres=60,numplots=5)\n",
    "\n",
    "# calculate some basic statistics\n",
    "fvar, maxarray, meanarray = stat_bands(objects,dobjects,bands_inlc)\n",
    "\n",
    "# combine different waveband into one array\n",
    "dat_notnormal = combine_bands(objects,bands_inlc)\n",
    "\n",
    "# Normalize the combinde array by maximum of brightness in a waveband after clipping outliers:\n",
    "dat = normalize_clipmax_objects(dat_notnormal,maxarray,band = 1)\n",
    "\n",
    "# Normalize the combinde array by mean brightness in a waveband after clipping outliers:\n",
    "datm = normalize_clipmax_objects(dat_notnormal,meanarray,band = 1)\n",
    "\n",
    "# shuffle data incase the ML routines are sensitive to order\n",
    "data,fzr,p = shuffle_datalabel(dat,flabels)\n",
    "fvar_arr,maximum_arr,average_arr = fvar[:,p],maxarray[:,p],meanarray[:,p]\n",
    "\n",
    "# Fix label of all changing looks and TDEs \n",
    "cl_labels = ['LaMassa 15','Green 22','Hon 22','Lyu 21','Sheng 20','MacLeod 19','MacLeod 16','Ruan 16','Yang 18','Lopez-Navas 22']\n",
    "for i,l in enumerate(fzr):\n",
    "    if l in cl_labels:\n",
    "        fzr[i] = 'CL AGN'\n",
    "    if l=='ZTF-Objname':\n",
    "        fzr[i] = 'TDE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3821ae",
   "metadata": {},
   "source": [
    "The combination of the tree bands into one longer arrays in order of increasing wavelength, can be seen as providing both the SED shape as well as variability in each from the light curve. Figure below demonstrates this as well as our normalization choice. We normalize the data in ZTF R band as it has a higher average numbe of visits compared to G and I band. We remove outliers before measuring the mean and max of the light curve and normalizing by it. This normalization can be skipped if one is mearly interested in comparing brightnesses of the data in this sample, but as dependence on flux is strong to look for variability and compare shapes of light curves a normalization helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e122f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.randint(np.shape(dat)[1])\n",
    "plt.figure(figsize=(18,4))\n",
    "plt.subplot(1,3,1)\n",
    "\n",
    "for i,l in enumerate(bands_inlc):\n",
    "    s = int(np.shape(dat)[1]/len(bands_inlc))\n",
    "    first = int(i*s)\n",
    "    last = first+s\n",
    "    plt.plot(np.linspace(first,last,s),dat_notnormal[r,first:last],'o',linestyle='--',label=l)\n",
    "plt.xlabel(r'Time_[w1,w2,w3]',size=15)\n",
    "plt.ylabel(r'Flux ($\\mu Jy$)',size=15)\n",
    "plt.legend(loc=2)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "for i,l in enumerate(bands_inlc):\n",
    "    s = int(np.shape(dat)[1]/len(bands_inlc))\n",
    "    first = int(i*s)\n",
    "    last = first+s\n",
    "    plt.plot(np.linspace(first,last,s),dat[r,first:last],'o',linestyle='--',label=l)\n",
    "plt.xlabel(r'Time_[w1,w2,w3]',size=15)\n",
    "plt.ylabel(r'Normalized Flux (max r band)',size=15)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "for i,l in enumerate(bands_inlc):\n",
    "    s = int(np.shape(dat)[1]/len(bands_inlc))\n",
    "    first = int(i*s)\n",
    "    last = first+s\n",
    "    plt.plot(np.linspace(first,last,s),datm[r,first:last],'o',linestyle='--',label=l)\n",
    "plt.xlabel(r'Time_[w1,w2,w3]',size=15)\n",
    "plt.ylabel(r'Normalized Flux (mean r band)',size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6697baa",
   "metadata": {},
   "source": [
    "Now we can train a UMAP with the processed data vectors above. Different choices for the number of neighbors, minimum distance and metric can be made and a parameter space can be explored. We show here our preferred combination given this data. We choose manhattan distance (also called [the L1 distance](https://en.wikipedia.org/wiki/Taxicab_geometry)) as it is optimal for the kind of grid we interpolated on, for instance we want the distance to not change if there are observations missing. Another metric appropriate for our purpose in time domain analysis is Dynamic Time Warping ([DTW](https://en.wikipedia.org/wiki/Dynamic_time_warping)), which is insensitive to a shift in time. This is helpful as we interpolate the observations onto a grid starting from time 0 and when discussing variability we care less about when it happens and more about whether and how strong it happened. As the measurement of the DTW distance takes longer compared to the other metrics we show examples here with manhattan and only show one example exploring the parameter space including a DTW metric in the last cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "markersize=200\n",
    "mapper = umap.UMAP(n_neighbors=10,min_dist=0.01,metric='manhattan',random_state=20).fit(data)\n",
    "\n",
    "ax1 = plt.subplot(1,3,2)\n",
    "ax1.set_title(r'mean brightness',size=20)\n",
    "cf = ax1.scatter(mapper.embedding_[:,0],mapper.embedding_[:,1],s=markersize,c=np.log10(np.nansum(meanarray,axis=0)),edgecolor='gray')\n",
    "plt.colorbar(cf)\n",
    "plt.axis('off')\n",
    "\n",
    "ax0 = plt.subplot(1,3,3)\n",
    "ax0.set_title(r'mean fractional variation',size=20)\n",
    "cf = ax0.scatter(mapper.embedding_[:,0],mapper.embedding_[:,1],s=markersize,c=stretch_small_values_arctan(np.nansum(fvar_arr,axis=0),factor=3),edgecolor='gray')\n",
    "plt.colorbar(cf)\n",
    "plt.axis('off')\n",
    "\n",
    "ax2 = plt.subplot(1,3,1)\n",
    "ax2.set_title('sample origin',size=20)\n",
    "for l,lab in enumerate(np.unique(fzr)):\n",
    "    u=(fzr[:]==lab)\n",
    "    cf = ax2.scatter(mapper.embedding_[u,0],mapper.embedding_[u,1],s=markersize,c=colors[l],alpha=0.5,edgecolor='gray',label=lab)\n",
    "plt.legend(fontsize=12)\n",
    "plt.colorbar(cf)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('umap-ztf.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88117992",
   "metadata": {},
   "source": [
    "The left panel is colorcoded by the origin of the sample. The middle panel shows the sum of mean brightnesses in three bands (arbitrary unit) demonstrating that after normalization we see no correlation with brightness. The panel on the right is color coded by a statistical measure of variability (i.e. the fractional variation [see here](https://ned.ipac.caltech.edu/level5/Sept01/Peterson2/Peter2_1.html)). As with the plotting above it is not easy to see all the data points and correlations in the next two cells measure probability of belonging to each original sample as well as the mean statistical property on an interpolated grid on this reduced 2D projected surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43256788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid\n",
    "grid_resolution = 25# Number of cells in the grid\n",
    "x_min, x_max = mapper.embedding_[:, 0].min(), mapper.embedding_[:, 0].max()\n",
    "y_min, y_max = mapper.embedding_[:, 1].min(), mapper.embedding_[:, 1].max()\n",
    "x_grid = np.linspace(x_min, x_max, grid_resolution)\n",
    "y_grid = np.linspace(y_min, y_max, grid_resolution)\n",
    "x_centers, y_centers = np.meshgrid(x_grid, y_grid)\n",
    "\n",
    "# Calculate mean property in each grid cell\n",
    "mean_property1,mean_property2 = np.zeros_like(x_centers),np.zeros_like(x_centers)\n",
    "propmean=stretch_small_values_arctan(np.nansum(meanarray,axis=0),factor=2)\n",
    "propfvar=stretch_small_values_arctan(np.nansum(fvar_arr,axis=0),factor=2)\n",
    "for i in range(grid_resolution - 1):\n",
    "    for j in range(grid_resolution - 1):\n",
    "        mask = (\n",
    "            (mapper.embedding_[:, 0] >= x_grid[i]) &\n",
    "            (mapper.embedding_[:, 0] < x_grid[i + 1]) &\n",
    "            (mapper.embedding_[:, 1] >= y_grid[j]) &\n",
    "            (mapper.embedding_[:, 1] < y_grid[j + 1])\n",
    "        )\n",
    "        if np.sum(mask) > 0:\n",
    "            mean_property1[j, i] = np.mean(propmean[mask])\n",
    "            mean_property2[j, i] = np.mean(propfvar[mask])\n",
    "            \n",
    "            \n",
    "plt.figure(figsize=(9,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('mean brightness')\n",
    "plt.contourf(x_centers, y_centers, mean_property1, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar()\n",
    "#plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('mean fractional variation')\n",
    "plt.contourf(x_centers, y_centers, mean_property2, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar()\n",
    "#plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 2D histogram\n",
    "hist, x_edges, y_edges = np.histogram2d(mapper.embedding_[:, 0], mapper.embedding_[:, 1], bins=12)\n",
    "\n",
    "# Calculate class probabilities for each bin\n",
    "class_probabilities = []\n",
    "\n",
    "for l,lab in enumerate(np.unique(fzr)):\n",
    "    u=(fzr[:]==lab)\n",
    "    hist_per_cluster, _, _ = np.histogram2d(mapper.embedding_[u,0], mapper.embedding_[u,1], bins=(x_edges, y_edges))\n",
    "    class_prob = hist_per_cluster / hist\n",
    "    class_probabilities.append(class_prob)\n",
    "    #print(lab, class_prob)\n",
    "\n",
    "labs = np.unique(fzr)\n",
    "plt.figure(figsize=(15,8))\n",
    "ax0 = plt.subplot(3,3,1)\n",
    "for i, prob in enumerate(class_probabilities):\n",
    "    plt.subplot(3,3,i+2)\n",
    "    plt.title(labs[i])\n",
    "    plt.contourf(x_edges[:-1], y_edges[:-1], prob.T, levels=20, alpha=0.8)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    u=(fzr[:]==labs[i])\n",
    "    cf = ax0.scatter(mapper.embedding_[u,0],mapper.embedding_[u,1],s=80,c=colors[i],alpha=0.5,edgecolor='gray',label=labs[i])\n",
    "ax0.legend(loc=4,fontsize=7)\n",
    "ax0.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79e206",
   "metadata": {},
   "source": [
    "Figure above shows how with ZTF light curves alone we can separate some of these AGN samples, where they have overlaps. We can do a similar exercise with other dimensionality reduction techniques. Below we show two SOMs one with normalized and another with no normalization. The advantage of Umaps to SOMs is that in practice you may change the parameters to separate classes of vastly different data points, as distance is preserved on a umap. On a SOM however only topology of higher dimensions is preserved and not distance hence, the change on the 2d grid does not need to be smooth and from one cell to next there might be larg jumps. On the other hand, an advantage of the SOM is that by definition it has a grid and no need for a posterior interpolation (as we did above) is needed to map more data or to measure probabilities, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sompy import * #using the SOMPY package from https://github.com/sevamoo/SOMPY\n",
    "msz0,msz1 = 12,12\n",
    "sm = sompy.SOMFactory.build(data, mapsize=[msz0,msz1], mapshape='planar', lattice='rect', initialization='pca') \n",
    "sm.train(n_job=4, shared_memory = 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sm.bmu_ind_to_xy(sm.project_data(data))\n",
    "x,y=np.zeros(len(a)),np.zeros(len(a))\n",
    "k=0\n",
    "for i in a:\n",
    "    x[k]=i[0]\n",
    "    y[k]=i[1]\n",
    "    k+=1    \n",
    "med_r=np.zeros([msz0,msz1])\n",
    "fvar_new = stretch_small_values_arctan(np.nansum(fvar_arr,axis=0),factor=1)\n",
    "for i in range(msz0):\n",
    "    for j in range(msz1):\n",
    "        unja=(x==i)&(y==j)\n",
    "        med_r[i,j]=(np.nanmedian(fvar_new[unja]))\n",
    "        \n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.subplot(1,4,1)\n",
    "plt.title('SDSS',fontsize=15)\n",
    "cf=plt.imshow(med_r,origin='lower',cmap='viridis')\n",
    "plt.axis('off')\n",
    "\n",
    "d = []\n",
    "for i,f in enumerate(fzr):\n",
    "    if f=='SDSS':\n",
    "        d.append(data[i,:])\n",
    "dsdss =np.array(d)\n",
    "asdss=sm.bmu_ind_to_xy(sm.project_data(dsdss))                   \n",
    "x,y=np.zeros(len(asdss)),np.zeros(len(asdss))\n",
    "k=0\n",
    "for i in asdss:\n",
    "    x[k]=i[0]\n",
    "    y[k]=i[1]\n",
    "    k+=1\n",
    "plt.plot(y,x,'rx',alpha=0.5)\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.title('WISE Variable',fontsize=15)\n",
    "cf=plt.imshow(med_r,origin='lower',cmap='viridis')\n",
    "plt.axis('off')\n",
    "d = []\n",
    "for i,f in enumerate(fzr):\n",
    "    if f=='WISE-Variable':\n",
    "        d.append(data[i,:])\n",
    "dwise =np.array(d)\n",
    "awise=sm.bmu_ind_to_xy(sm.project_data(dwise))                   \n",
    "x,y=np.zeros(len(awise)),np.zeros(len(awise))\n",
    "k=0\n",
    "for i in awise:\n",
    "    x[k]=i[0]\n",
    "    y[k]=i[1]\n",
    "    k+=1\n",
    "plt.plot(y,x,'rx',alpha=0.5)\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.title('CICCO 2019',fontsize=15)\n",
    "cf=plt.imshow(med_r,origin='lower',cmap='viridis')\n",
    "plt.axis('off')\n",
    "d = []\n",
    "for i,f in enumerate(fzr):\n",
    "    if f=='Cicco19':\n",
    "        d.append(data[i,:])\n",
    "dcic =np.array(d)\n",
    "acic=sm.bmu_ind_to_xy(sm.project_data(dcic))                   \n",
    "x,y=np.zeros(len(acic)),np.zeros(len(acic))\n",
    "k=0\n",
    "for i in acic:\n",
    "    x[k]=i[0]\n",
    "    y[k]=i[1]\n",
    "    k+=1\n",
    "plt.plot(y,x,'rx',alpha=0.5)\n",
    "\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.title('Changing Look AGNs',fontsize=15)\n",
    "cf=plt.imshow(med_r,origin='lower',cmap='viridis')\n",
    "plt.axis('off')\n",
    "d = []\n",
    "for i,f in enumerate(fzr):\n",
    "    if f=='CL AGN':\n",
    "        d.append(data[i,:])\n",
    "dcic =np.array(d)\n",
    "acic=sm.bmu_ind_to_xy(sm.project_data(dcic))                   \n",
    "x,y=np.zeros(len(acic)),np.zeros(len(acic))\n",
    "k=0\n",
    "for i in acic:\n",
    "    x[k]=i[0]\n",
    "    y[k]=i[1]\n",
    "    k+=1\n",
    "plt.plot(y,x,'rx',alpha=0.5)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4461b23",
   "metadata": {},
   "source": [
    "The above SOMs are colored by the mean fractional variation of the lightcurves in all bands (a measure of AGN variability). The crosses are different samples mapped to the trained SOM to see if they are distinguishable on a normalized lightcurve som."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221652aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data incase the ML routines are sensitive to order\n",
    "data,fzr,p = shuffle_datalabel(dat_notnormal,flabels)\n",
    "fvar_arr,maximum_arr,average_arr = fvar[:,p],maxarray[:,p],meanarray[:,p]\n",
    "cl_labels = ['LaMassa 15','Green 22','Hon 22','Lyu 21','Sheng 20','MacLeod 19','MacLeod 16','Ruan 16','Yang 18','Lopez-Navas 22']\n",
    "for i,l in enumerate(fzr):\n",
    "    if l in cl_labels:\n",
    "        fzr[i] = 'CL AGN'\n",
    "    if l=='ZTF-Objname':\n",
    "        fzr[i] = 'TDE'       \n",
    "sm = sompy.SOMFactory.build(data, mapsize=[10,10], mapshape='planar', lattice='rect', initialization='pca') \n",
    "sm.train(n_job=4, shared_memory = 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sm.bmu_ind_to_xy(sm.project_data(data))\n",
    "x,y=np.zeros(len(a)),np.zeros(len(a))\n",
    "k=0\n",
    "for i in a:\n",
    "    x[k]=i[0]\n",
    "    y[k]=i[1]\n",
    "    k+=1    \n",
    "med_r=np.zeros([msz0,msz1])\n",
    "fvar_new = stretch_small_values_arctan(np.nansum(maximum_arr,axis=0),factor=0.5)\n",
    "for i in range(msz0):\n",
    "    for j in range(msz1):\n",
    "        unja=(x==i)&(y==j)\n",
    "        med_r[i,j]=(np.nanmedian(fvar_new[unja]))\n",
    "        \n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.subplot(1,4,1)\n",
    "plt.title(r'SDSS',fontsize=15)\n",
    "cf=plt.imshow(med_r,origin='lower',cmap='viridis')\n",
    "plt.axis('off')\n",
    "\n",
    "d = []\n",
    "for i,f in enumerate(fzr):\n",
    "    if f=='SDSS':\n",
    "        d.append(data[i,:])\n",
    "dsdss =np.array(d)\n",
    "asdss=sm.bmu_ind_to_xy(sm.project_data(dsdss))                   \n",
    "x,y=np.zeros(len(asdss)),np.zeros(len(asdss))\n",
    "k=0\n",
    "for i in asdss:\n",
    "    x[k]=i[0]\n",
    "    y[k]=i[1]\n",
    "    k+=1\n",
    "plt.plot(y,x,'rx',alpha=0.5)\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.title(r'WISE Variable',fontsize=15)\n",
    "cf=plt.imshow(med_r,origin='lower',cmap='viridis')\n",
    "plt.axis('off')\n",
    "d = []\n",
    "for i,f in enumerate(fzr):\n",
    "    if f=='WISE-Variable':\n",
    "        d.append(data[i,:])\n",
    "dwise =np.array(d)\n",
    "awise=sm.bmu_ind_to_xy(sm.project_data(dwise))                   \n",
    "x,y=np.zeros(len(awise)),np.zeros(len(awise))\n",
    "k=0\n",
    "for i in awise:\n",
    "    x[k]=i[0]\n",
    "    y[k]=i[1]\n",
    "    k+=1\n",
    "plt.plot(y,x,'rx',alpha=0.5)\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.title(r'CICCO 2019',fontsize=15)\n",
    "cf=plt.imshow(med_r,origin='lower',cmap='viridis')\n",
    "plt.axis('off')\n",
    "d = []\n",
    "for i,f in enumerate(fzr):\n",
    "    if f=='Cicco19':\n",
    "        d.append(data[i,:])\n",
    "dcic =np.array(d)\n",
    "acic=sm.bmu_ind_to_xy(sm.project_data(dcic))                   \n",
    "x,y=np.zeros(len(acic)),np.zeros(len(acic))\n",
    "k=0\n",
    "for i in acic:\n",
    "    x[k]=i[0]\n",
    "    y[k]=i[1]\n",
    "    k+=1\n",
    "plt.plot(y,x,'rx',alpha=0.5)\n",
    "\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.title(r'Changing Look AGNs',fontsize=15)\n",
    "cf=plt.imshow(med_r,origin='lower',cmap='viridis')\n",
    "plt.axis('off')\n",
    "d = []\n",
    "for i,f in enumerate(fzr):\n",
    "    if f=='CL AGN':\n",
    "        d.append(data[i,:])\n",
    "dcic =np.array(d)\n",
    "acic=sm.bmu_ind_to_xy(sm.project_data(dcic))                   \n",
    "x,y=np.zeros(len(acic)),np.zeros(len(acic))\n",
    "k=0\n",
    "for i in acic:\n",
    "    x[k]=i[0]\n",
    "    y[k]=i[1]\n",
    "    k+=1\n",
    "plt.plot(y,x,'rx',alpha=0.5)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faface90",
   "metadata": {},
   "source": [
    "skipping the normalization of lightcurves, can show for example how the Cicco et al. 2019 sample, from the 3year VLT observations of the COSMOS field are all fainter compared to the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d75030",
   "metadata": {},
   "source": [
    "# Repeating the above, this time with Panstarrs observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db7f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_inlc = ['panstarrs g','panstarrs r','panstarrs i','panstarrs z', 'panstarrs y']\n",
    "objects,dobjects,flabels = unify_lc(parallel_df_lc,bands_inlc,xres=30)\n",
    "\n",
    "# calculate some basic statistics\n",
    "fvar, maxarray, meanarray = stat_bands(objects,dobjects,bands_inlc)\n",
    "\n",
    "# combine different waveband into one array\n",
    "dat_notnormal = combine_bands(objects,bands_inlc)\n",
    "\n",
    "# Normalize the combinde array by maximum of brightness in a waveband after clipping outliers:\n",
    "dat = normalize_clipmax_objects(dat_notnormal,maxarray,band = 1)\n",
    "\n",
    "# Normalize the combinde array by mean brightness in a waveband after clipping outliers:\n",
    "datm = normalize_clipmax_objects(dat_notnormal,meanarray,band = 1)\n",
    "\n",
    "# shuffle data incase the ML routines are sensitive to order\n",
    "data,fzr,p = shuffle_datalabel(dat,flabels)\n",
    "fvar_arr,maximum_arr,average_arr = fvar[:,p],maxarray[:,p],meanarray[:,p]\n",
    "\n",
    "# Fix label of all changing looks and TDEs \n",
    "cl_labels = ['LaMassa 15','Green 22','Hon 22','Lyu 21','Sheng 20','MacLeod 19','MacLeod 16','Ruan 16','Yang 18','Lopez-Navas 22']\n",
    "for i,l in enumerate(fzr):\n",
    "    if l in cl_labels:\n",
    "        fzr[i] = 'CL AGN'\n",
    "    if l=='ZTF-Objname':\n",
    "        fzr[i] = 'TDE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "markersize=200\n",
    "mapper = umap.UMAP(n_neighbors=50,min_dist=0.5,metric='manhattan',random_state=20).fit(data)\n",
    "\n",
    "ax1 = plt.subplot(1,3,2)\n",
    "ax1.set_title(r'mean brightness',size=20)\n",
    "cf = ax1.scatter(mapper.embedding_[:,0],mapper.embedding_[:,1],s=markersize,c=np.log10(np.nansum(meanarray,axis=0)),edgecolor='gray')\n",
    "plt.colorbar(cf)\n",
    "plt.axis('off')\n",
    "\n",
    "ax0 = plt.subplot(1,3,3)\n",
    "ax0.set_title(r'mean fractional variation',size=20)\n",
    "cf = ax0.scatter(mapper.embedding_[:,0],mapper.embedding_[:,1],s=markersize,c=stretch_small_values_arctan(np.nansum(fvar_arr,axis=0),factor=2),edgecolor='gray')\n",
    "plt.colorbar(cf)\n",
    "plt.axis('off')\n",
    "\n",
    "ax2 = plt.subplot(1,3,1)\n",
    "ax2.set_title('sample origin',size=20)\n",
    "for l,lab in enumerate(np.unique(fzr)):\n",
    "    u=(fzr[:]==lab)\n",
    "    cf = ax2.scatter(mapper.embedding_[u,0],mapper.embedding_[u,1],s=markersize,c=colors[l],alpha=0.6,edgecolor='gray',label=lab)\n",
    "plt.legend(fontsize=12)\n",
    "plt.colorbar(cf)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('umap-Panstarrs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db356d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 2D histogram\n",
    "hist, x_edges, y_edges = np.histogram2d(mapper.embedding_[:, 0], mapper.embedding_[:, 1], bins=12)\n",
    "\n",
    "# Calculate class probabilities for each bin\n",
    "class_probabilities = []\n",
    "\n",
    "for l,lab in enumerate(np.unique(fzr)):\n",
    "    u=(fzr[:]==lab)\n",
    "    hist_per_cluster, _, _ = np.histogram2d(mapper.embedding_[u,0], mapper.embedding_[u,1], bins=(x_edges, y_edges))\n",
    "    class_prob = hist_per_cluster / hist\n",
    "    class_probabilities.append(class_prob)\n",
    "    #print(lab, class_prob)\n",
    "\n",
    "labs = np.unique(fzr)\n",
    "plt.figure(figsize=(15,8))\n",
    "ax0 = plt.subplot(3,3,1)\n",
    "for i, prob in enumerate(class_probabilities):\n",
    "    plt.subplot(3,3,i+2)\n",
    "    plt.title(labs[i])\n",
    "    plt.contourf(x_edges[:-1], y_edges[:-1], prob.T, levels=20, alpha=0.8)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    u=(fzr[:]==labs[i])\n",
    "    cf = ax0.scatter(mapper.embedding_[u,0],mapper.embedding_[u,1],s=80,c=colors[i],alpha=0.5,edgecolor='gray',label=labs[i])\n",
    "ax0.legend(loc=4,fontsize=7)\n",
    "ax0.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e8316",
   "metadata": {},
   "source": [
    "# ZTF + WISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b82e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_inlc = ['zg','zr','zi','W1','W2']\n",
    "objects,dobjects,flabels = unify_lc(parallel_df_lc,bands_inlc,xres=30,numplots=3)\n",
    "# calculate some basic statistics\n",
    "fvar, maxarray, meanarray = stat_bands(objects,dobjects,bands_inlc)\n",
    "dat_notnormal = combine_bands(objects,bands_inlc)\n",
    "dat = normalize_clipmax_objects(dat_notnormal,maxarray,band = -1)\n",
    "data,fzr,p = shuffle_datalabel(dat,flabels)\n",
    "fvar_arr,maximum_arr,average_arr = fvar[:,p],maxarray[:,p],meanarray[:,p]\n",
    "\n",
    "# Fix label of all changing looks and TDEs \n",
    "cl_labels = ['LaMassa 15','Green 22','Hon 22','Lyu 21','Sheng 20','MacLeod 19','MacLeod 16','Ruan 16','Yang 18','Lopez-Navas 22']\n",
    "for i,l in enumerate(fzr):\n",
    "    if l in cl_labels:\n",
    "        fzr[i] = 'CL AGN'\n",
    "    if l=='ZTF-Objname':\n",
    "        fzr[i] = 'TDE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "markersize=200\n",
    "mapper = umap.UMAP(n_neighbors=200,min_dist=1,metric='manhattan',random_state=12).fit(data)\n",
    "\n",
    "ax1 = plt.subplot(1,3,2)\n",
    "ax1.set_title(r'mean brightness',size=20)\n",
    "cf = ax1.scatter(mapper.embedding_[:,0],mapper.embedding_[:,1],s=markersize,c=stretch_small_values_arctan(np.nansum(meanarray,axis=0),factor=10),edgecolor='gray')\n",
    "plt.colorbar(cf)\n",
    "plt.axis('off')\n",
    "\n",
    "ax0 = plt.subplot(1,3,3)\n",
    "ax0.set_title(r'mean fractional variation',size=20)\n",
    "cf = ax0.scatter(mapper.embedding_[:,0],mapper.embedding_[:,1],s=markersize,c=stretch_small_values_arctan(np.nansum(fvar_arr[-2:-1,:],axis=0),factor=10),edgecolor='gray')\n",
    "plt.colorbar(cf)\n",
    "plt.axis('off')\n",
    "\n",
    "ax2 = plt.subplot(1,3,1)\n",
    "ax2.set_title('sample origin',size=20)\n",
    "for l,lab in enumerate(np.unique(fzr)):\n",
    "    u=(fzr[:]==lab)\n",
    "    cf = ax2.scatter(mapper.embedding_[u,0],mapper.embedding_[u,1],s=markersize,c=colors[l],alpha=0.6,edgecolor='gray',label=lab)\n",
    "plt.legend(fontsize=12)\n",
    "plt.colorbar(cf)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 2D histogram\n",
    "hist, x_edges, y_edges = np.histogram2d(mapper.embedding_[:, 0], mapper.embedding_[:, 1], bins=12)\n",
    "\n",
    "# Calculate class probabilities for each bin\n",
    "class_probabilities = []\n",
    "\n",
    "for l,lab in enumerate(np.unique(fzr)):\n",
    "    u=(fzr[:]==lab)\n",
    "    hist_per_cluster, _, _ = np.histogram2d(mapper.embedding_[u,0], mapper.embedding_[u,1], bins=(x_edges, y_edges))\n",
    "    class_prob = hist_per_cluster / hist\n",
    "    class_probabilities.append(class_prob)\n",
    "    #print(lab, class_prob)\n",
    "\n",
    "labs = np.unique(fzr)\n",
    "plt.figure(figsize=(15,8))\n",
    "ax0 = plt.subplot(3,3,1)\n",
    "for i, prob in enumerate(class_probabilities):\n",
    "    plt.subplot(3,3,i+2)\n",
    "    plt.title(labs[i])\n",
    "    plt.contourf(x_edges[:-1], y_edges[:-1], prob.T, levels=20, alpha=0.8)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    u=(fzr[:]==labs[i])\n",
    "    cf = ax0.scatter(mapper.embedding_[u,0],mapper.embedding_[u,1],s=80,c=colors[i],alpha=0.5,edgecolor='gray',label=labs[i])\n",
    "ax0.legend(loc=4,fontsize=7)\n",
    "ax0.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd68b459",
   "metadata": {},
   "source": [
    "# Wise alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_inlc = ['W1','W2']\n",
    "objects,dobjects,flabels = unify_lc(parallel_df_lc,bands_inlc,xres=30)\n",
    "# calculate some basic statistics\n",
    "fvar, maxarray, meanarray = stat_bands(objects,dobjects,bands_inlc)\n",
    "dat_notnormal = combine_bands(objects,bands_inlc)\n",
    "dat = normalize_clipmax_objects(dat_notnormal,maxarray,band = -1)\n",
    "data,fzr,p = shuffle_datalabel(dat,flabels)\n",
    "fvar_arr,maximum_arr,average_arr = fvar[:,p],maxarray[:,p],meanarray[:,p]\n",
    "\n",
    "# Fix label of all changing looks and TDEs \n",
    "cl_labels = ['LaMassa 15','Green 22','Hon 22','Lyu 21','Sheng 20','MacLeod 19','MacLeod 16','Ruan 16','Yang 18','Lopez-Navas 22']\n",
    "for i,l in enumerate(fzr):\n",
    "    if l in cl_labels:\n",
    "        fzr[i] = 'CL AGN'\n",
    "    if l=='ZTF-Objname':\n",
    "        fzr[i] = 'TDE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "markersize=200\n",
    "mapper = umap.UMAP(n_neighbors=200,min_dist=1,metric='manhattan',random_state=5).fit(data)\n",
    "\n",
    "ax1 = plt.subplot(1,3,2)\n",
    "ax1.set_title(r'mean brightness',size=20)\n",
    "cf = ax1.scatter(mapper.embedding_[:,0],mapper.embedding_[:,1],s=markersize,c=np.log10(np.nansum(meanarray,axis=0)),edgecolor='gray')\n",
    "plt.colorbar(cf)\n",
    "plt.axis('off')\n",
    "\n",
    "ax0 = plt.subplot(1,3,3)\n",
    "ax0.set_title(r'mean fractional variation',size=20)\n",
    "cf = ax0.scatter(mapper.embedding_[:,0],mapper.embedding_[:,1],s=markersize,c=stretch_small_values_arctan(np.nansum(fvar_arr[-2:-1,:],axis=0),factor=4),edgecolor='gray')\n",
    "plt.colorbar(cf)\n",
    "plt.axis('off')\n",
    "\n",
    "ax2 = plt.subplot(1,3,1)\n",
    "ax2.set_title('sample origin',size=20)\n",
    "for l,lab in enumerate(np.unique(fzr)):\n",
    "    u=(fzr[:]==lab)\n",
    "    cf = ax2.scatter(mapper.embedding_[u,0],mapper.embedding_[u,1],s=markersize,c=colors[l],alpha=0.6,edgecolor='gray',label=lab)\n",
    "plt.legend(fontsize=12)\n",
    "plt.colorbar(cf)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('umap-wise.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a60fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 2D histogram\n",
    "hist, x_edges, y_edges = np.histogram2d(mapper.embedding_[:, 0], mapper.embedding_[:, 1], bins=12)\n",
    "\n",
    "# Calculate class probabilities for each bin\n",
    "class_probabilities = []\n",
    "\n",
    "for l,lab in enumerate(np.unique(fzr)):\n",
    "    u=(fzr[:]==lab)\n",
    "    hist_per_cluster, _, _ = np.histogram2d(mapper.embedding_[u,0], mapper.embedding_[u,1], bins=(x_edges, y_edges))\n",
    "    class_prob = hist_per_cluster / hist\n",
    "    class_probabilities.append(class_prob)\n",
    "    #print(lab, class_prob)\n",
    "\n",
    "labs = np.unique(fzr)\n",
    "plt.figure(figsize=(15,8))\n",
    "ax0 = plt.subplot(3,3,1)\n",
    "for i, prob in enumerate(class_probabilities):\n",
    "    plt.subplot(3,3,i+2)\n",
    "    plt.title(labs[i])\n",
    "    plt.contourf(x_edges[:-1], y_edges[:-1], prob.T, levels=20, alpha=0.8)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    u=(fzr[:]==labs[i])\n",
    "    cf = ax0.scatter(mapper.embedding_[u,0],mapper.embedding_[u,1],s=80,c=colors[i],alpha=0.5,edgecolor='gray',label=labs[i])\n",
    "ax0.legend(loc=4,fontsize=7)\n",
    "ax0.axis('off')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('wise.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d941a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "markersize=200\n",
    "\n",
    "mapper = umap.UMAP(n_neighbors=5,min_dist=0.01,metric='euclidean',random_state=20).fit(data)\n",
    "ax0 = plt.subplot(2,2,1)\n",
    "ax0.set_title(r'Euclidean Distance, min_d=0.01, n_neighbors=5',size=20)\n",
    "for l,lab in enumerate(np.unique(fzr)):\n",
    "    u=(fzr[:]==lab)\n",
    "    cf = ax0.scatter(mapper.embedding_[u,0],mapper.embedding_[u,1],s=markersize,c=colors[l],alpha=0.6,edgecolor='gray',label=lab)\n",
    "plt.legend(fontsize=12)\n",
    "plt.axis('off')\n",
    "\n",
    "mapper = umap.UMAP(n_neighbors=50,min_dist=0.5,metric='manhattan',random_state=20).fit(data)\n",
    "ax0 = plt.subplot(2,2,2)\n",
    "ax0.set_title(r'Manhattan Distance, min_d=0.5, n_neighbors=50',size=20)\n",
    "for l,lab in enumerate(np.unique(fzr)):\n",
    "    u=(fzr[:]==lab)\n",
    "    cf = ax0.scatter(mapper.embedding_[u,0],mapper.embedding_[u,1],s=markersize,c=colors[l],alpha=0.6,edgecolor='gray',label=lab)\n",
    "plt.legend(fontsize=12)\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "mapperg = umap.UMAP(n_neighbors=50,min_dist=0.5,metric=dtw_distance,random_state=20).fit(data) #this distance takes long\n",
    "ax2 = plt.subplot(2,2,3)\n",
    "ax2.set_title(r'DTW Distance, min_d=0.5,n_neighbors=50',size=20)\n",
    "for l,lab in enumerate(np.unique(fzr)):\n",
    "    u=(fzr[:]==lab)\n",
    "    cf = ax2.scatter(mapperg.embedding_[u,0],mapperg.embedding_[u,1],s=markersize,c=colors[l],alpha=0.6,edgecolor='gray',label=lab)\n",
    "plt.legend(fontsize=12)\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "mapper = umap.UMAP(n_neighbors=7,min_dist=0.1,metric='manhattan',random_state=20).fit(data)\n",
    "ax0 = plt.subplot(2,2,4)\n",
    "ax0.set_title(r'Manhattan Distance, min_d=0.1, n_neighbors=7',size=20)\n",
    "for l,lab in enumerate(np.unique(fzr)):\n",
    "    u=(fzr[:]==lab)\n",
    "    cf = ax0.scatter(mapper.embedding_[u,0],mapper.embedding_[u,1],s=markersize,c=colors[l],alpha=0.6,edgecolor='gray',label=lab)\n",
    "plt.legend(fontsize=12)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717115ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "source_map": [
   12,
   22,
   66,
   70,
   74,
   80,
   108,
   112,
   124,
   128,
   156,
   160,
   166,
   193,
   197,
   228,
   232,
   261,
   265,
   304,
   332,
   336,
   343,
   435,
   439,
   453,
   545,
   549,
   553,
   582,
   612,
   640,
   644,
   663,
   692,
   720,
   724,
   743,
   773,
   804,
   847
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}