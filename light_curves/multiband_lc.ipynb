{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make multiwavelength light curves using archival data\n",
    "***\n",
    "\n",
    "## Learning Goals    \n",
    "By the end of this tutorial, you will be able to:\n",
    " - automatically load a catalog of sources\n",
    " - automatically search NASA and non-NASA resources for light curves\n",
    " - store light curves in a Pandas multiindex dataframe\n",
    " - plot all light curves on the same plot\n",
    " \n",
    " \n",
    "## Introduction:\n",
    " - A user has a sample of interesting targets for which they would like to see a plot of available archival light curves.  We start with a small set of changing look AGN from Yang et al., 2018, which are automatically downloaded. Changing look AGN are cases where the broad emission lines appear or disappear (and not just that the flux is variable). \n",
    " - We model light curve plots after van Velzen et al. 2021.  We search through a curated list of time-domain NASA holdings as well as non-NASA sources.  HEASARC catalogs used are Fermi and Beppo-Sax, IRSA catalogs used are ZTF and WISE, and MAST catalogs used are Pan-Starrs, TESS, Kepler, and K2.  Non-NASA sources are Gaia and IceCube. This list is generalized enough to include many types of targets to make this notebook interesting for many types of science.  All of these time-domain archives are searched in an automated fashion using astroquery or APIs.\n",
    " - Light curve data storage is a tricky problem.  Currently we are using a multi-index Pandas dataframe, as the best existing choice for right now.  One downside is that we need to manually track the units of flux and time instead of relying on an astropy storage scheme which would be able to do some of the units worrying for us (even astropy can't do all magnitude to flux conversions).  Astropy does not currently have a good option for multi-band light curve storage.\n",
    " - We intend to explore a ML classifier for these changing look AGN light curves.\n",
    " \n",
    "## Input:\n",
    " - choose from a list of known changing look AGN from the literature\n",
    " \n",
    "  OR - \n",
    " - input your own sample\n",
    "\n",
    "## Output:\n",
    " - an archival optical + IR + neutrino light curve\n",
    " \n",
    "## Non-standard Imports:\n",
    "- `acstools` to work with HST magnitude to flux conversion\n",
    "- `astropy` to work with coordinates/units and data structures\n",
    "- `astroquery` to interface with archives APIs\n",
    "- `hpgeom` to locate coordinates in HEALPix space\n",
    "- `lightkurve` to search TESSS, Kepler, and K2 archives\n",
    "- `pyarrow` to work with Parquet files for WISE and ZTF\n",
    "- `s3fs` to connect to AWS S3 buckets\n",
    "- `urllib` to handle archive searches with website interface\n",
    "\n",
    "## Authors:\n",
    "Jessica Krick, Shoubaneh Hemmati, Andreas Faisst, Troy Raen, Brigitta Sipocz, Dave Shupe\n",
    "\n",
    "## Acknowledgements:\n",
    "Suvi Gezari, Antara Basu-zych,Stephanie LaMassa\\\n",
    "MAST, HEASARC, & IRSA Fornax teams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/fkiwy/unTimely_Catalog_explorer (from -r requirements.txt (line 7))\n",
      "  Cloning https://github.com/fkiwy/unTimely_Catalog_explorer to /tmp/pip-req-build-2z168iwl\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/fkiwy/unTimely_Catalog_explorer /tmp/pip-req-build-2z168iwl\n",
      "  Resolved https://github.com/fkiwy/unTimely_Catalog_explorer to commit 954b06b5dc91c207c335e652083327c4ec136dd9\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting git+https://github.com/sevamoo/SOMPY (from -r requirements.txt (line 20))\n",
      "  Cloning https://github.com/sevamoo/SOMPY to /tmp/pip-req-build-3b8soaev\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/sevamoo/SOMPY /tmp/pip-req-build-3b8soaev\n",
      "  Resolved https://github.com/sevamoo/SOMPY to commit 6aca604b06e5eea1391ecf507810c7aabafc3f8b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting acstools\n",
      "  Using cached acstools-3.6.1-py3-none-any.whl (102 kB)\n",
      "Requirement already satisfied: astropy in /srv/conda/envs/notebook/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (5.1)\n",
      "Collecting astroquery\n",
      "  Using cached astroquery-0.4.6-py3-none-any.whl (4.5 MB)\n",
      "Collecting hpgeom\n",
      "  Using cached hpgeom-1.0.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (151 kB)\n",
      "Collecting lightkurve\n",
      "  Using cached lightkurve-2.4.1-py3-none-any.whl (265 kB)\n",
      "Requirement already satisfied: matplotlib in /srv/conda/envs/notebook/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (3.5.2)\n",
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (1.22.4)\n",
      "Requirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (1.4.2)\n",
      "Collecting pyvo\n",
      "  Using cached pyvo-1.4.2-py3-none-any.whl (888 kB)\n",
      "Collecting reproject\n",
      "  Using cached reproject-0.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "Requirement already satisfied: s3fs in /srv/conda/envs/notebook/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (2022.5.0)\n",
      "Requirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (4.64.0)\n",
      "Collecting wget\n",
      "  Using cached wget-3.2-py3-none-any.whl\n",
      "Collecting alerce\n",
      "  Using cached alerce-1.2.0-py3-none-any.whl (11 kB)\n",
      "Collecting umap-learn[plot]\n",
      "  Using cached umap_learn-0.5.4-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.9/site-packages (from acstools->-r requirements.txt (line 4)) (2.28.0)\n",
      "Requirement already satisfied: PyYAML in /srv/conda/envs/notebook/lib/python3.9/site-packages (from acstools->-r requirements.txt (line 4)) (5.4.1)\n",
      "Requirement already satisfied: pyerfa>=2.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from astropy->-r requirements.txt (line 5)) (2.0.0.1)\n",
      "Requirement already satisfied: packaging>=19.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from astropy->-r requirements.txt (line 5)) (21.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.3.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from astroquery->-r requirements.txt (line 6)) (4.11.1)\n",
      "Collecting html5lib>=0.999\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Collecting keyring>=4.0\n",
      "  Using cached keyring-24.2.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pillow in /srv/conda/envs/notebook/lib/python3.9/site-packages (from unTimely-Catalog-explorer==1.1.0->-r requirements.txt (line 7)) (9.1.1)\n",
      "Collecting uncertainties>=3.1.4\n",
      "  Using cached uncertainties-3.1.7-py2.py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: patsy>=0.5.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from lightkurve->-r requirements.txt (line 9)) (0.5.2)\n",
      "Collecting memoization>=0.3.1\n",
      "  Using cached memoization-0.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from lightkurve->-r requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: bokeh>=2.0.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from lightkurve->-r requirements.txt (line 9)) (2.4.3)\n",
      "Collecting oktopus>=0.1.2\n",
      "  Using cached oktopus-0.1.2-py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=1.7 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from lightkurve->-r requirements.txt (line 9)) (1.8.1)\n",
      "Requirement already satisfied: urllib3>=1.23 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from lightkurve->-r requirements.txt (line 9)) (1.26.9)\n",
      "Collecting fbpca>=1.0\n",
      "  Using cached fbpca-1.0-py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 10)) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 10)) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 10)) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 10)) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 10)) (4.33.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 12)) (2022.1)\n",
      "Requirement already satisfied: cloudpickle in /srv/conda/envs/notebook/lib/python3.9/site-packages (from reproject->-r requirements.txt (line 14)) (2.1.0)\n",
      "Requirement already satisfied: zarr in /srv/conda/envs/notebook/lib/python3.9/site-packages (from reproject->-r requirements.txt (line 14)) (2.11.3)\n",
      "Collecting astropy-healpix>=0.6\n",
      "  Using cached astropy_healpix-1.0.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
      "Requirement already satisfied: dask[array]>=2021.8 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from reproject->-r requirements.txt (line 14)) (2022.6.0)\n",
      "Requirement already satisfied: fsspec in /srv/conda/envs/notebook/lib/python3.9/site-packages (from reproject->-r requirements.txt (line 14)) (2022.5.0)\n",
      "Requirement already satisfied: aiohttp<=4 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from s3fs->-r requirements.txt (line 15)) (3.8.1)\n",
      "Requirement already satisfied: aiobotocore~=2.3.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from s3fs->-r requirements.txt (line 15)) (2.3.3)\n",
      "Requirement already satisfied: numba>=0.51.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from umap-learn[plot]->-r requirements.txt (line 19)) (0.53.1)\n",
      "Collecting tbb>=2019.0\n",
      "  Using cached tbb-2021.10.0-py2.py3-none-manylinux1_x86_64.whl (4.4 MB)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.10-py3-none-any.whl\n",
      "Requirement already satisfied: colorcet in /srv/conda/envs/notebook/lib/python3.9/site-packages (from umap-learn[plot]->-r requirements.txt (line 19)) (3.0.0)\n",
      "Requirement already satisfied: seaborn in /srv/conda/envs/notebook/lib/python3.9/site-packages (from umap-learn[plot]->-r requirements.txt (line 19)) (0.11.2)\n",
      "Requirement already satisfied: scikit-image in /srv/conda/envs/notebook/lib/python3.9/site-packages (from umap-learn[plot]->-r requirements.txt (line 19)) (0.19.2)\n",
      "Requirement already satisfied: holoviews in /srv/conda/envs/notebook/lib/python3.9/site-packages (from umap-learn[plot]->-r requirements.txt (line 19)) (1.14.9)\n",
      "Requirement already satisfied: datashader in /srv/conda/envs/notebook/lib/python3.9/site-packages (from umap-learn[plot]->-r requirements.txt (line 19)) (0.14.0)\n",
      "Collecting numexpr>=2.5\n",
      "  Using cached numexpr-2.8.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from aiobotocore~=2.3.0->s3fs->-r requirements.txt (line 15)) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.24.22,>=1.24.21 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from aiobotocore~=2.3.0->s3fs->-r requirements.txt (line 15)) (1.24.21)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from aiobotocore~=2.3.0->s3fs->-r requirements.txt (line 15)) (1.14.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from aiohttp<=4->s3fs->-r requirements.txt (line 15)) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from aiohttp<=4->s3fs->-r requirements.txt (line 15)) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from aiohttp<=4->s3fs->-r requirements.txt (line 15)) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from aiohttp<=4->s3fs->-r requirements.txt (line 15)) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from aiohttp<=4->s3fs->-r requirements.txt (line 15)) (2.0.12)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from aiohttp<=4->s3fs->-r requirements.txt (line 15)) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from aiohttp<=4->s3fs->-r requirements.txt (line 15)) (1.7.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from beautifulsoup4>=4.3.2->astroquery->-r requirements.txt (line 6)) (2.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from bokeh>=2.0.0->lightkurve->-r requirements.txt (line 9)) (4.2.0)\n",
      "Requirement already satisfied: tornado>=5.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from bokeh>=2.0.0->lightkurve->-r requirements.txt (line 9)) (6.1)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from bokeh>=2.0.0->lightkurve->-r requirements.txt (line 9)) (3.1.2)\n",
      "Requirement already satisfied: partd>=0.3.10 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from dask[array]>=2021.8->reproject->-r requirements.txt (line 14)) (1.2.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from dask[array]>=2021.8->reproject->-r requirements.txt (line 14)) (0.11.2)\n",
      "Requirement already satisfied: six>=1.9 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from html5lib>=0.999->astroquery->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /srv/conda/envs/notebook/lib/python3.9/site-packages (from html5lib>=0.999->astroquery->-r requirements.txt (line 6)) (0.5.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.11.4 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from keyring>=4.0->astroquery->-r requirements.txt (line 6)) (4.11.4)\n",
      "Collecting jeepney>=0.4.2\n",
      "  Using cached jeepney-0.8.0-py3-none-any.whl (48 kB)\n",
      "Collecting jaraco.classes\n",
      "  Using cached jaraco.classes-3.3.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting SecretStorage>=3.2\n",
      "  Using cached SecretStorage-3.3.3-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from numba>=0.51.2->umap-learn[plot]->-r requirements.txt (line 19)) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.9/site-packages (from numba>=0.51.2->umap-learn[plot]->-r requirements.txt (line 19)) (62.3.4)\n",
      "Collecting autograd\n",
      "  Using cached autograd-1.6.2-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from pynndescent>=0.5->umap-learn[plot]->-r requirements.txt (line 19)) (1.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from requests->acstools->-r requirements.txt (line 4)) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from requests->acstools->-r requirements.txt (line 4)) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-learn>=0.24.0->lightkurve->-r requirements.txt (line 9)) (3.1.0)\n",
      "Requirement already satisfied: future in /srv/conda/envs/notebook/lib/python3.9/site-packages (from uncertainties>=3.1.4->lightkurve->-r requirements.txt (line 9)) (0.18.2)\n",
      "Requirement already satisfied: param>=1.7.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from colorcet->umap-learn[plot]->-r requirements.txt (line 19)) (1.12.1)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from colorcet->umap-learn[plot]->-r requirements.txt (line 19)) (0.4.6)\n",
      "Requirement already satisfied: datashape>=0.5.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from datashader->umap-learn[plot]->-r requirements.txt (line 19)) (0.5.4)\n",
      "Requirement already satisfied: xarray>=0.9.6 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from datashader->umap-learn[plot]->-r requirements.txt (line 19)) (2022.3.0)\n",
      "Requirement already satisfied: panel>=0.8.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from holoviews->umap-learn[plot]->-r requirements.txt (line 19)) (0.13.1)\n",
      "Requirement already satisfied: pyviz-comms>=0.7.4 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from holoviews->umap-learn[plot]->-r requirements.txt (line 19)) (2.2.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image->umap-learn[plot]->-r requirements.txt (line 19)) (2022.5.4)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image->umap-learn[plot]->-r requirements.txt (line 19)) (2.19.3)\n",
      "Requirement already satisfied: networkx>=2.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image->umap-learn[plot]->-r requirements.txt (line 19)) (2.8.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image->umap-learn[plot]->-r requirements.txt (line 19)) (1.3.0)\n",
      "Requirement already satisfied: asciitree in /srv/conda/envs/notebook/lib/python3.9/site-packages (from zarr->reproject->-r requirements.txt (line 14)) (0.3.3)\n",
      "Requirement already satisfied: fasteners in /srv/conda/envs/notebook/lib/python3.9/site-packages (from zarr->reproject->-r requirements.txt (line 14)) (0.17.3)\n",
      "Requirement already satisfied: numcodecs>=0.6.4 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from zarr->reproject->-r requirements.txt (line 14)) (0.9.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from botocore<1.24.22,>=1.24.21->aiobotocore~=2.3.0->s3fs->-r requirements.txt (line 15)) (1.0.0)\n",
      "Requirement already satisfied: distributed==2022.6.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from dask[array]>=2021.8->reproject->-r requirements.txt (line 14)) (2022.6.0)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from distributed==2022.6.0->dask[array]>=2021.8->reproject->-r requirements.txt (line 14)) (2.4.0)\n",
      "Requirement already satisfied: click>=6.6 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from distributed==2022.6.0->dask[array]>=2021.8->reproject->-r requirements.txt (line 14)) (8.1.3)\n",
      "Requirement already satisfied: psutil>=5.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from distributed==2022.6.0->dask[array]>=2021.8->reproject->-r requirements.txt (line 14)) (5.9.1)\n",
      "Requirement already satisfied: locket>=1.0.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from distributed==2022.6.0->dask[array]>=2021.8->reproject->-r requirements.txt (line 14)) (1.0.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from distributed==2022.6.0->dask[array]>=2021.8->reproject->-r requirements.txt (line 14)) (1.7.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from distributed==2022.6.0->dask[array]>=2021.8->reproject->-r requirements.txt (line 14)) (1.0.4)\n",
      "Requirement already satisfied: zict>=0.1.3 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from distributed==2022.6.0->dask[array]>=2021.8->reproject->-r requirements.txt (line 14)) (2.2.0)\n",
      "Requirement already satisfied: multipledispatch>=0.4.7 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from datashape>=0.5.1->datashader->umap-learn[plot]->-r requirements.txt (line 19)) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from importlib-metadata>=4.11.4->keyring>=4.0->astroquery->-r requirements.txt (line 6)) (3.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from Jinja2>=2.9->bokeh>=2.0.0->lightkurve->-r requirements.txt (line 9)) (2.1.1)\n",
      "Requirement already satisfied: markdown in /srv/conda/envs/notebook/lib/python3.9/site-packages (from panel>=0.8.0->holoviews->umap-learn[plot]->-r requirements.txt (line 19)) (3.3.7)\n",
      "Requirement already satisfied: bleach in /srv/conda/envs/notebook/lib/python3.9/site-packages (from panel>=0.8.0->holoviews->umap-learn[plot]->-r requirements.txt (line 19)) (5.0.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from SecretStorage>=3.2->keyring>=4.0->astroquery->-r requirements.txt (line 6)) (37.0.2)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-10.1.0-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=4.0->astroquery->-r requirements.txt (line 6)) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=4.0->astroquery->-r requirements.txt (line 6)) (2.21)\n",
      "Requirement already satisfied: heapdict in /srv/conda/envs/notebook/lib/python3.9/site-packages (from zict>=0.1.3->distributed==2022.6.0->dask[array]>=2021.8->reproject->-r requirements.txt (line 14)) (1.0.1)\n",
      "Building wheels for collected packages: unTimely-Catalog-explorer, SOMPY\n",
      "  Building wheel for unTimely-Catalog-explorer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unTimely-Catalog-explorer: filename=unTimely_Catalog_explorer-1.1.0-py3-none-any.whl size=13217 sha256=a103384293ed076eea282d91694a5e0dfa745a3b893414285e76edc1e32b6034\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6km8pn6r/wheels/10/84/2d/f0d1765ad9b0ecd47489f350ba673b6fe9f9ba3bdf07e10e1b\n",
      "  Building wheel for SOMPY (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for SOMPY: filename=SOMPY-1.1.1-py3-none-any.whl size=29185 sha256=ca8ff477bae850a7be72af5d7616ecfdd164d1d7924afb338220d270ff32416c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6km8pn6r/wheels/fe/70/5e/97cde95fb0e8c026f22e1b7102b1603f7cc3560438466a978b\n",
      "Successfully built unTimely-Catalog-explorer SOMPY\n",
      "Installing collected packages: wget, tbb, fbpca, uncertainties, numexpr, more-itertools, memoization, jeepney, html5lib, hpgeom, autograd, oktopus, jaraco.classes, SOMPY, SecretStorage, pyvo, pynndescent, astropy-healpix, alerce, acstools, umap-learn, reproject, keyring, unTimely-Catalog-explorer, astroquery, lightkurve\n",
      "Successfully installed SOMPY-1.1.1 SecretStorage-3.3.3 acstools-3.6.1 alerce-1.2.0 astropy-healpix-1.0.0 astroquery-0.4.6 autograd-1.6.2 fbpca-1.0 hpgeom-1.0.0 html5lib-1.1 jaraco.classes-3.3.0 jeepney-0.8.0 keyring-24.2.0 lightkurve-2.4.1 memoization-0.4.0 more-itertools-10.1.0 numexpr-2.8.7 oktopus-0.1.2 pynndescent-0.5.10 pyvo-1.4.2 reproject-0.12.0 tbb-2021.10.0 umap-learn-0.5.4 unTimely-Catalog-explorer-1.1.0 uncertainties-3.1.7 wget-3.2\n"
     ]
    }
   ],
   "source": [
    "#ensure all dependencies are installed\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from math import ceil\n",
    "\n",
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# local code imports\n",
    "sys.path.append('code/')\n",
    "from panstarrs import panstarrs_get_lightcurves\n",
    "from gaia_functions import Gaia_get_lightcurve\n",
    "from HCV_functions import HCV_get_lightcurves\n",
    "from icecube_functions import icecube_get_lightcurve\n",
    "from sample_selection import get_lamassa_sample, get_macleod16_sample, get_ruan_sample, get_macleod19_sample, get_sheng_sample, \\\n",
    "    get_green_sample, get_lyu_sample, get_lopeznavas_sample, get_hon_sample, get_yang_sample,  get_SDSS_sample, clean_sample\n",
    "from data_structures import MultiIndexDFObject\n",
    "from heasarc_functions import HEASARC_get_lightcurves\n",
    "from TESS_Kepler_functions import TESS_Kepler_get_lightcurves\n",
    "from plot_functions import create_figures\n",
    "# Note: WISE and ZTF data are temporarily located in a non-public AWS S3 bucket. It is automatically\n",
    "# available from the Fornax SMCE, but will require user credentials for access outside the SMCE.\n",
    "from WISE_functions import WISE_get_lightcurves\n",
    "from ztf_functions import ZTF_get_lightcurve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when using multiprocessing, \"spawn\" new processes because it uses less memory and is thread safe\n",
    "# in particular, this is required for pd.read_parquet (used by ZTF_get_lightcurve)\n",
    "# https://stackoverflow.com/questions/64095876/multiprocessing-fork-vs-spawn\n",
    "mp.set_start_method(\"spawn\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Sample\n",
    " We define here a \"gold\" sample of spectroscopically confirmed changing look AGN and quasars. This sample includes both objects which change from type 1 to type 2 and also the opposite.  Future studies may want to treat these as seperate objects or seperate QSOs from AGN.\n",
    " \n",
    " Bibcodes for the samples used are listed next to their functions for reference.  \n",
    " \n",
    " Functions used to grab the samples from the papers use Astroquery, NED, SIMBAD, Vizier, and in a few cases grab the tables from the html versions of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing Look AGN- Yang et al:  31\n",
      "SDSS Quasar: 0\n",
      "after duplicates removal, sample size: 30\n"
     ]
    }
   ],
   "source": [
    "#build up the sample\n",
    "coords =[]\n",
    "labels = []\n",
    "\n",
    "#choose your own adventure:\n",
    "\n",
    "#get_lamassa_sample(coords, labels)  #2015ApJ...800..144L\n",
    "#get_macleod16_sample(coords, labels) #2016MNRAS.457..389M\n",
    "#get_ruan_sample(coords, labels) #2016ApJ...826..188R\n",
    "#get_macleod19_sample(coords, labels)  #2019ApJ...874....8M\n",
    "#get_sheng_sample(coords, labels)  #2020ApJ...889...46S\n",
    "#get_green_sample(coords, labels)  #2022ApJ...933..180G\n",
    "#get_lyu_sample(coords, labels)  #z32022ApJ...927..227L\n",
    "#get_lopeznavas_sample(coords, labels)  #2022MNRAS.513L..57L\n",
    "#get_hon_sample(coords, labels)  #2022MNRAS.511...54H\n",
    "get_yang_sample(coords, labels)   #2018ApJ...862..109Y\n",
    "\n",
    "#now get some \"normal\" QSOs for use in the classifier\n",
    "#there are ~500K of these, so choose the number based on\n",
    "#a balance between speed of running the light curves and whatever \n",
    "#the ML algorithms would like to have\n",
    "\n",
    "#num_normal_QSO = 50\n",
    "#get_SDSS_sample(coords, labels, num_normal_QSO)\n",
    "\n",
    "# remove duplicates and attach an objectid to the coords\n",
    "coords_list, labels_list = clean_sample(coords, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Build your own Sample\n",
    "\n",
    "To build your own sample, you can follow the examples of functions above to grab coordinates from your favorite literature resource, \n",
    "\n",
    "or\n",
    "\n",
    "You can use [astropy's read](https://docs.astropy.org/en/stable/io/ascii/read.html) function to read in an input table\n",
    "and then convert that table into a list of [skycoords](https://docs.astropy.org/en/stable/api/astropy.coordinates.SkyCoord.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find light curves for these targets in NASA catalogs\n",
    "  - We search a curated list of time-domain catalogs from all NASA astrophysics archives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize Pandas MultiIndex data frame for storing the light curves\n",
    "df_lc = MultiIndexDFObject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 HEASARC: FERMI & Beppo SAX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_serial = time.time()\n",
    "\n",
    "#what is the size of error_radius for the fermi catalog that we will accept for our cross-matching?\n",
    "#in degrees; chosen based on histogram of all values for these catalogs\n",
    "max_fermi_error_radius = str(1.0)  \n",
    "max_sax_error_radius = str(3.0)\n",
    "\n",
    "#list of missions to query and their corresponding error radii\n",
    "heasarc_cat = [\"FERMIGTRIG\", \"SAXGRBMGRB\"]\n",
    "error_radius = [max_fermi_error_radius , max_sax_error_radius]\n",
    "\n",
    "\n",
    "#go out and find all light curves in the above curated list which match our target positions\n",
    "df_lc_fermi = HEASARC_get_lightcurves(coords_list, labels_list, heasarc_cat, error_radius)\n",
    "df_lc.append(df_lc_fermi)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 IRSA: ZTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use the nworkers arg to control the amount of parallelization in the data loading step\n",
    "df_lc_ZTF = ZTF_get_lightcurve(coords_list, labels_list, nworkers=6)\n",
    "\n",
    "#add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_ZTF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 IRSA: WISE\n",
    "\n",
    "- use the unWISE light curves catalog which ties together all WISE & NEOWISE 2010 - 2020 epochs.  Specifically it combined all observations at a single epoch to achieve deeper mag limits than individual observations alone.\n",
    "- [Meisner et al., 2023, 2023AJ....165...36M](https://ui.adsabs.harvard.edu/abs/2023AJ....165...36M/abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bandlist = ['W1', 'W2']\n",
    "WISE_radius = 1.0 * u.arcsec\n",
    "\n",
    "df_lc_WISE = WISE_get_lightcurves(coords_list, labels_list, WISE_radius, bandlist)\n",
    "\n",
    "#add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_WISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 MAST: Pan-STARRS\n",
    "Query the Pan-STARRS API; based on this [example](https://ps1images.stsci.edu/ps1_dr2_api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do a panstarrs search\n",
    "panstarrs_radius = 1.0/3600.0    # search radius = 1 arcsec\n",
    "df_lc_panstarrs = panstarrs_get_lightcurves(coords_list, labels_list, panstarrs_radius)\n",
    "\n",
    "#add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_panstarrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 MAST: Asteroid Terrestrial-impact Last Alert System (ATLAS)\n",
    " - All-sky stellar reference catalog \n",
    " -  MAST hosts this catalog but there are three barriers to using it\n",
    "     1. it is unclear if the MAST [holdings]( https://archive.stsci.edu/hlsp/atlas-refcat2#section-a737bc3e-2d56-4827-9ab4-838fbf8d67c1) include the individual epoch photometry and \n",
    "     2. it is only accessible with casjobs, not through python notebooks.  \n",
    "     3. magnitude range (g, r, i) < 19mag makes it not relevant for this use case\n",
    " \n",
    "One path forward if this catalog becomes scientifically interesting is to put in a MAST helpdesk ticket to see if 1) they do have the light curves, and 2) they could switch the catalog to a searchable with python version.  There are some ways of [accessing casjobs with python](<https://github.com/spacetelescope/notebooks/blob/master/notebooks/MAST/HSC/HCV_CASJOBS/HCV_casjobs_demo.ipynb), but not this particular catalog.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 MAST: TESS, Kepler and K2\n",
    " - use [`lightKurve`](https://docs.lightkurve.org/index.html) to search all 3 missions and download light curves\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#go get the lightcurves using lightkurve\n",
    "TESS_radius = 1.0  #arcseconds\n",
    "df_lc_TESS = TESS_Kepler_get_lightcurves(coords_list, labels_list, TESS_radius)\n",
    "\n",
    "#add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_TESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 MAST: HCV\n",
    " - [hubble catalog of variables](https://archive.stsci.edu/hlsp/hcv) \n",
    " - using [this notebook](https://archive.stsci.edu/hst/hsc/help/HCV/HCV_API_demo.html) as a reference to search and download light curves via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do an HCV search\n",
    "HCV_radius = 1.0/3600.0 # radius = 1 arcsec\n",
    "df_lc_HCV = HCV_get_lightcurves(coords_list, labels_list, HCV_radius)\n",
    "\n",
    "#add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_HCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find light curves for these targets in relevant, non-NASA catalogs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Gaia \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaiastarttime = time.time()\n",
    "df_lc_gaia = Gaia_get_lightcurve(coords_list,  labels_list , verbose = 1)\n",
    "\n",
    "#add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_gaia)\n",
    "\n",
    "print('gaia search took:', time.time() - gaiastarttime, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ASAS-SN (all sky automated survey for supernovae) \n",
    "- Has a [website](https://asas-sn.osu.edu/photometry) that can be manually searched; but no API which would allow automatic searches from within this notebook\n",
    "- Magnitude range of this survey is not consistent with the magnitude range of our CLAGN.  If this catalog becomes scientifically interesting, one path forward would be to ask ASAS-SN team about implementing an API\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Icecube Neutrinos\n",
    "\n",
    "There are several [catalogs](https://icecube.wisc.edu/data-releases/2021/01/all-sky-point-source-icecube-data-years-2008-2018) (basically one for each year of IceCube data from 2008 - 2018). The following code creates a large catalog by combining\n",
    "all the yearly catalogs.\n",
    "The IceCube catalog contains Neutrino detections with associated energy and time and approximate direction (which is uncertain by half-degree scales....). Usually, for active events only one or two Neutrinos are detected, which makes matching quite different compared to \"photons\". For our purpose, we will list the top 3 events in energy that are within a given distance to the target.\n",
    "\n",
    "This time series (time vs. neutrino energy) information is similar to photometry. We choose to storing time and energy in our data structure, leaving error = 0. What is __not__ stored in this format is the distance or angular uncertainty of the event direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lc_icecube = icecube_get_lightcurve(coords_list , labels_list ,\n",
    "                                   icecube_select_topN = 3,\n",
    "                                   path = \"./data/\",\n",
    "                                   verbose = 1)\n",
    "\n",
    "#add the resulting dataframe to all other archives\n",
    "df_lc.append(df_lc_icecube)\n",
    "end_serial = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmarking\n",
    "print('total time for serial archive calls is ', end_serial - start_serial, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parallel Processing the archive calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some variables in case the above serial cells are not run\n",
    "max_fermi_error_radius = str(1.0)  \n",
    "max_sax_error_radius = str(3.0)\n",
    "heasarc_cat = [\"FERMIGTRIG\", \"SAXGRBMGRB\"]\n",
    "error_radius = [max_fermi_error_radius , max_sax_error_radius]\n",
    "bandlist = [\"W1\", \"W2\"]\n",
    "wise_radius = 1.0 * u.arcsec\n",
    "panstarrs_radius = 1.0 / 3600.0  # search radius = 1 arcsec\n",
    "lk_radius = 1.0  # arcseconds\n",
    "hcv_radius = 1.0 / 3600.0  # radius = 1 arcsec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of workers needed\n",
    "n_workers = 8  # this should equal the total number of archives called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ZTF call can be parallelized internally, separate from the multiprocessing pool launched below.\n",
    "# these parallelizations are mutually exclusive, so we must turn off the internal parallelization.\n",
    "ztf_nworkers = None\n",
    "\n",
    "# the ZTF call is relatively slow compared to other archives.\n",
    "# if you want to query for a large number of objects, it will be faster to call ZTF individually\n",
    "# (code above) and use the internal parallelization. try 8-12 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parallel_starttime = time.time()\n",
    "\n",
    "# start a multiprocessing pool and run all the archive queries\n",
    "parallel_df_lc = MultiIndexDFObject()  # to collect the results\n",
    "callback = parallel_df_lc.append  # will be called once on the result returned by each archive\n",
    "with mp.Pool(processes=n_workers) as pool:\n",
    "\n",
    "    # start the processes that call the archives\n",
    "    pool.apply_async(\n",
    "        Gaia_get_lightcurve, (coords_list,  labels_list , 1), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        HEASARC_get_lightcurves, (coords_list, labels_list, heasarc_cat, error_radius), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        HCV_get_lightcurves, (coords_list, labels_list, hcv_radius), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        icecube_get_lightcurve, (coords_list, labels_list, 3, \"./data/\", 1), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        panstarrs_get_lightcurves, (coords_list, labels_list, panstarrs_radius), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        TESS_Kepler_get_lightcurves, (coords_list, labels_list, lk_radius), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        WISE_get_lightcurves, (coords_list, labels_list, wise_radius, bandlist), callback=callback\n",
    "    )\n",
    "    pool.apply_async(\n",
    "        ZTF_get_lightcurve, (coords_list, labels_list, ztf_nworkers), callback=callback\n",
    "    )\n",
    "\n",
    "    pool.close()  # signal that no more jobs will be submitted to the pool\n",
    "    pool.join()  # wait for all jobs to complete, including the callback\n",
    "\n",
    "parallel_endtime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long did parallel processing take?\n",
    "# and look at the results\n",
    "print('parallel processing took', parallel_endtime - parallel_starttime, 's')\n",
    "parallel_df_lc.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data for future use with ML notebook\n",
    "#parquet_savename = 'data/df_lc_090723_yang.parquet'\n",
    "#parallel_df_lc.data.to_parquet(parquet_savename)\n",
    "#print(\"file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could load a previously saved file in order to plot\n",
    "#parquet_loadname = 'data/df_lc_090723_yang.parquet'\n",
    "#parallel_df_lc = MultiIndexDFObject()\n",
    "#parallel_df_lc.data = pd.read_parquet(parquet_loadname)\n",
    "#print(\"file loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make plots of luminosity as a function of time\n",
    "Model plots after [van Velzen et al., 2021](https://arxiv.org/pdf/2111.09391.pdf).\n",
    "\n",
    "__Note__ that in the following, we can either plot the results from `df_lc` (from the serial call) or `parallel_df_lc` (from the parallel call). By default (see next cell) the output of the parallel call is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "_ = create_figures(coords_list = coords_list ,\n",
    "                   df_lc = parallel_df_lc, # either df_lc (serial call) or parallel_df_lc (parallel call)\n",
    "                   show_nbr_figures = 5,\n",
    "                   save_output_dir = \"/home/jovyan/fornax-demo-notebooks/light_curves/data\" ,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "This work made use of:\n",
    "\n",
    "- Astroquery; Ginsburg et al., 2019, 2019AJ....157...98G\n",
    "\n",
    "- Astropy; Astropy Collaboration 2022, Astropy Collaboration 2018, Astropy Collaboration 2013, 2022ApJ...935..167A, 2018AJ....156..123A, 2013A&A...558A..33A\n",
    "\n",
    "- Lightkurve; Lightkurve Collaboration 2018, 2018ascl.soft12013L\n",
    "\n",
    "- acstools; https://zenodo.org/record/7406933#.ZBH1HS-B0eY\n",
    "\n",
    "- unWISE light curves; Meisner et al., 2023, 2023AJ....165...36M\n",
    "\n",
    "- Alerce; Forster et al., 2021, 2021AJ....161..242F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
